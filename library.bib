@article{Abbeel2005,
abstract = {Kalman filters are a workhorse of robotics and are routinely used in state-estimation problems. However, their performance critically depends on a large number of modeling parameters which can be very difficult to obtain, and are often set via significant manual tweaking and at a great cost of engineering time. In this paper, we propose a method for automatically learning the noise parameters of a Kalman filter. We also demonstrate on a commercial wheeled rover that our Kalman filter’s learned noise covariance parameters—obtained quickly and fully automatically—significantly outperform an earlier, carefully and laboriously hand-designed one.},
author = {Abbeel, Pieter and Coates, Adam and Montemerlo, Michael and Ng, Andrew Y and Thrun, Sebastian},
doi = {10.1.1.76.759},
file = {:home/jkaiser/Downloads/rss05-discriminativeKF (1).pdf:pdf},
journal = {Proceedings of Robotics: Science and Systems I},
pages = {289--296},
title = {{Discriminative Training of Kalman Filters}},
year = {2005}
}
@article{Armesto2007,
abstract = {This paper presents a tracking system for ego-motion estimation which fuses vision and inertial measurements using EKF and UKF (Extended and Unscented Kalman Filters), where a comparison of their performance has been done. It also considers the multi-rate nature of the sensors: inertial sensing is sampled at a fast sampling frequency while the sampling frequency of vision is lower. the proposed approach uses a constant linear acceleration model and constant angular velocity model based on quaternions, which yields a non-linear model for states and a linear model in measurement equations. Results show that a significant improvement is obtained on the estimation when fusing both measurements with respect to just vision or just inertial measurements. It is also shown that the proposed system can estimate fast-motions even when vision system fails. Moreover, a study of the influence of the noise covariance is also performed, which aims to select their appropriate values at the tuning process. The setup is an end-effector mounted camera, which allow us to pre-define basic rotational and translational motions for validating results.},
author = {Armesto, L. and Tornero, J. and Vincze, M.},
doi = {10.1177/0278364907079283},
file = {:home/jkaiser/Downloads/The International Journal of Robotics Research-2007-Armesto-577-89.pdf:pdf},
isbn = {0278364907079},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {multi-rate systems,sensor,vision and inertial},
number = {6},
pages = {577--589},
title = {{Fast Ego-motion Estimation with Multi-rate Fusion of Inertial and Vision}},
volume = {26},
year = {2007}
}
@article{Bibuli2007,
author = {Bibuli, Marco and Caccia, Massimo and Lapierre, Lionel},
doi = {10.1002/rob},
file = {:home/jkaiser/Downloads/Sibley\_et\_al-2010-Journal\_of\_Field\_Robotics.pdf:pdf},
isbn = {9783902661623},
issn = {14746670},
journal = {IFAC Proceedings Volumes (IFAC-PapersOnline)},
keywords = {Backstepping,Non-linear control,Path-following},
number = {PART 1},
pages = {81--86},
title = {{Sliding Window Filter with Application to Planetary Landing}},
volume = {7},
year = {2007}
}
@article{Dias2007,
author = {Dias, J. and Vinzce, M. and Corke, P. and Lobo, J.},
doi = {10.1177/0278364907079903},
file = {:home/jkaiser/Downloads/The International Journal of Robotics Research-2007-Dias-515-7.pdf:pdf},
isbn = {0278364907079},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
number = {6},
pages = {515--517},
title = {{Editorial: Special Issue: 2nd Workshop on Integration of Vision and Inertial Sensors}},
volume = {26},
year = {2007}
}
@article{DongSi2012,
abstract = {This paper focuses on motion estimation using inertial measurements and observations of naturally occurring point features. To date, this task has primarily been addressed using filtering methods, which track the system state starting from known initial conditions. However, when no prior knowledge of the initial system state is available, (e.g., at the onset of the system\&apos;s operation), the existing approaches are not applicable. To address this problem, in this work we present algorithms for computing the system\&apos;s observable quantities (platform attitude and velocity, feature positions, and IMU-camera calibration) directly from the sensor measurements, without any prior knowledge. A key contribution of this work is a convex-optimization based algorithm for computing the rotation matrix between the camera and IMU. We show that once this rotation matrix has been computed, all remaining quantities can be determined by solving a quadratically constrained least-squares problem. To increase their accuracy, the initial estimates are refined by an iterative maximum-likelihood estimator. View full abstract},
author = {Dong-Si, Tue Cuong and Mourikis, Anastasios I.},
doi = {10.1109/IROS.2012.6386235},
file = {:home/jkaiser/Downloads/DongSi2012IROS.pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1064--1071},
pmid = {6386235},
title = {{Estimator initialization in vision-aided inertial navigation with unknown camera-IMU calibration}},
year = {2012}
}
@article{Engel2014,
author = {Engel, Jakob and Cremers, Daniel},
file = {:home/jkaiser/Downloads/engel14ras.pdf:pdf},
keywords = {ar,drone,monocular slam,quadrocopter,scale estimation,visual navigation,visual slam},
title = {{Scale-Aware Navigation of a Low-Cost Quadrocopter with a Monocular Camera}},
year = {2014}
}
@article{Engel2013,
abstract = {We propose a fundamentally novel approach to real-time visual odometry for a monocular camera. It allows to ben- efit from the simplicity and accuracy of dense tracking – which does not depend on visual features – while running in real-time on a CPU. The key idea is to continuously estimate a semi-dense inverse depth map for the current frame, which in turn is used to track the motion of the camera using dense image alignment. More specifically, we estimate the depth of all pixels which have a non-negligible image gradient. Each estimate is represented as a Gaussian probability distribution over the inverse depth. We propagate this information over time, and update it with new measurements as new images arrive. In terms of tracking accuracy and computational speed, the proposed method compares favorably to both state-of-the-art dense and feature-based visual odometry and SLAM algorithms. As our method runs in real-time on a CPU, it is of large practical value for robotics and augmented reality applications. 1.Towards},
author = {Engel, Jakob and Sturm, Jurgen and Cremers, Daniel},
doi = {10.1109/ICCV.2013.183},
file = {:home/jkaiser/Downloads/engel2013iccv.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {SLAM,dense,monocular,stereo,visual odometry},
pages = {1449--1456},
title = {{Semi-dense visual odometry for a monocular camera}},
year = {2013}
}
@article{Euston2008,
abstract = {This paper considers the question of using a nonlinear complementary filter for attitude estimation of fixed-wing unmanned aerial vehicle (UAV) given only measurements from a low-cost inertial measurement unit. A nonlinear complementary filter is proposed that combines accelerometer output for low frequency attitude estimation with integrated gyrometer output for high frequency estimation. The raw accelerometer output includes a component corresponding to airframe acceleration, occurring primarily when the aircraft turns, as well as the gravitational acceleration that is required for the filter. The airframe acceleration is estimated using a simple centripetal force model (based on additional airspeed measurements), augmented by a first order dynamic model for angle-of-attack, and used to obtain estimates of the gravitational direction independent of the airplane manoeuvres. Experimental results are provided on a real-world data set and the performance of the filter is evaluated against the output from a full GPS/INS that was available for the data set.},
author = {Euston, Mark and Coote, Paul and Mahony, Robert and Kim, Jonghyuk and Hamel, Tarek},
doi = {10.1109/IROS.2008.4650766},
file = {:home/jkaiser/Downloads/attitudeEstimation.pdf:pdf},
isbn = {9781424420582},
journal = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
pages = {340--345},
title = {{A complementary filter for attitude estimation of a fixed-wing UAV}},
year = {2008}
}
@inproceedings{Faessler2015,
author = {Faessler, Matthias and Fontana, Flavio and Forster, Christian and Scaramuzza, Davide},
booktitle = {International Conference on Robotics \& Automation},
file = {:home/jkaiser/Downloads/ICRA15\_Faessler.pdf:pdf},
title = {{Automatic Re-Initialization and Failure Recovery for Aggressive Flight with a Monocular Vision-Based Quadrotor}},
year = {2015}
}
@book{Farrell2008,
abstract = {Design Cutting-Edge Aided Navigation Systems for Advanced Commercial \& Military Applications Aided Navigation is a design-oriented textbook and guide to building aided navigation systems for smart cars, precision farming vehicles, smart weapons, unmanned aircraft, mobile robots, and other advanced applications. The navigation guide contains two parts explaining the essential theory, concepts, and tools, as well as the methodology in aided navigation case studies with sufficient detail to serve as the basis for application-oriented analysis and design. Filled with detailed illustrations and examples, this expert design tool takes you step-by-step through coordinate systems, deterministic and stochastic modeling, optimal estimation, and navigation system design. Authoritative and comprehensive, Aided Navigation features:End-of-chapter exercises throughout Part I In-depth case studies of aided navigation systems Numerous Matlab-based examples Appendices define notation, review linear algebra, and discuss GPS receiver interfacing Source code and sensor data to support examples is available through the publisher-supported websiteInside this Complete Guide to Designing Aided Navigation Systems • Aided Navigation Theory: Introduction to Aided Navigation • Coordinate Systems • Deterministic Modeling • Stochastic Modeling • Optimal Estimation • Navigation System Design • Navigation Case Studies: Global Positioning System (GPS) • GPS-Aided Encoder • Attitude and Heading Reference System • GPS-Aided Inertial Navigation System (INS) • Acoustic Ranging and Doppler-Aided INS},
author = {Farrell, Jay},
doi = {10.1036/0071493298},
file = {:home/jkaiser/Downloads/2008\_Aided.Navigation.GPS.with.High.Rate.Sensors\_Kalman.pdf:pdf},
isbn = {0071642668},
pages = {530},
publisher = {McGraw-Hill},
title = {{Aided Navigation: GPS with High Rate Sensors}},
url = {http://books.google.com/books?id=yNujEvIMszYC\&pgis=1},
year = {2008}
}

@inproceedings{Forster2015,
author = {C. ,Forster and L., Carlone and F., Dellaert and D., Scaramuzza},
booktitle = {Robotics: Science and Systems (RSS)},
title = {{IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation}},
year = {2015}
}



@article{Gemeiner2007,
abstract = {For mobile robotics, head gear in augmented reality (AR) applications or computer vision, it is essential to continuously estimate the egomotion and the structure of the environment. This paper presents the system developed in the SmartTracking project, which simultaneously integrates visual and inertial sensors in a combined estimation scheme. The sparse structure estimation is based on the detection of corner features in the environment. From a single known starting position, the system can move into an unknown environment. The vision and inertial data are fused, and the performance of both Unscented Kalman filter and Extended Kalman filter are compared for this task. The filters are designed to handle asynchronous input from visual and inertial sensors, which typically operate at different and possibly varying rates. Additionally, a bank of Extended Kalman filters, one per corner feature, is used to estimate the position and the quality of structure points and to include them into the structure estimation process. The system is demonstrated on a mobile robot executing known motions, such that the estimation of the egomotion in an unknown environment can be compared to ground truth.},
author = {Gemeiner, P. and Einramhof, P. and Vincze, M.},
doi = {10.1177/0278364907080058},
file = {:home/jkaiser/Downloads/The International Journal of Robotics Research-2007-Gemeiner-591-605.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {localization,mapping,sensor fusion},
number = {6},
pages = {591--605},
title = {{Simultaneous Motion and Structure Estimation by Fusion of Inertial and Vision Data}},
volume = {26},
year = {2007}
}
@article{Hamel2006,
abstract = {This paper considers the question of obtaining high quality attitude estimates from typical low cost inertial measurement units for applications in control of unmanned aerial vehicles. A nonlinear complimentary filter exploiting the structure of special orthogonal group S0(3) is proposed. The filter is expressed explicitly in terms of direct and untreated measurements. For a typical low cost inertial measurement where two inertial directions are measured (gravitational and magnetic fields) along with angular velocity, it is shown that the filter is well conditioned. If only a single direction is available (typically the gravitational field) along with angular velocity, it is shown that the full gyro bias vector is correctly estimated and that the estimated orientation converges to a set consistent with the measurements. Experimental results, for flight data from the HoverEyecopy UAV, demonstrate the efficiency of the proposed filter},
author = {Hamel, T. and Mahony, R.},
doi = {10.1109/ROBOT.2006.1642025},
file = {:home/jkaiser/Downloads/attitudeEstimate.pdf:pdf},
isbn = {0-7803-9505-0},
issn = {1050-4729},
journal = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},
number = {May},
pages = {2170--2175},
title = {{Attitude estimation on SO[3] based on direct inertial measurements}},
year = {2006}
}
@article{Hartley1997,
abstract = {The fundamental matrix is a basic tool in the analysis of scenes
taken with two uncalibrated cameras, and the eight-point algorithm is a
frequently cited method for computing the fundamental matrix from a set
of eight or more point matches. It has the advantage of simplicity of
implementation. The prevailing view is, however, that it is extremely
susceptible to noise and hence virtually useless for most purposes. This
paper challenges that view, by showing that by preceding the algorithm
with a very simple normalization (translation and scaling) of the
coordinates of the matched points, results are obtained comparable with
the best iterative algorithms. This improved performance is justified by
theory and verified by extensive experiments on real images},
author = {Hartley, Richard I.},
doi = {10.1109/34.601246},
file = {:home/jkaiser/Downloads/refined8Pt.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Condition number,Eight-point algorithm,Epipolar structure,Fundamental matrix,Stereo vision},
number = {6},
pages = {580--593},
title = {{In defense of the eight-point algorithm}},
volume = {19},
year = {1997}
}
@article{Hbf2015,
author = {Hbf, Karlsruhe and Zeit, Datum and Hbf, Karlsruhe and Hbf, Darmstadt},
file = {:home/jkaiser/Downloads/99K73P.pdf:pdf},
pages = {2--4},
title = {{Fahrkarte Europa-Spez.Frankr.}},
year = {2015}
}
@article{Huang2009,
abstract = {This paper addresses two key limitations of the unscented Kalman filter (UKF) when applied to the simultaneous localization and mapping (SLAM) problem: the cubic, in the number of states, computational complexity, and the inconsistency of the state estimates. In particular, we introduce a new sampling strategy that minimizes the linearization error and whose computational complexity is constant (i.e., independent of the size of the state vector). As a result, the overall computational complexity of UKF-based SLAM becomes of the same order as that of the extended Kalman filter (EKF) when applied to SLAM. Furthermore, we investigate the observability properties of the linear-regression-based model employed by the UKF, and propose a new algorithm, termed the observability-constrained (OC)-UKF, that improves the consistency of the state estimates. The superior performance of the OC-UKF compared to the standard UKF and its robustness to large linearization errors are validated by extensive simulations.},
author = {Huang, Guoquan P. and Mourikis, Anastasios I. and Roumeliotis, Stergios I.},
doi = {10.1109/ROBOT.2009.5152793},
file = {:home/jkaiser/Downloads/ICRA09-OC-UKF.pdf:pdf},
isbn = {9781424427895},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4401--4408},
title = {{On the complexity and consistency of UKF-based SLAM}},
year = {2009}
}
@article{Jones2011,
abstract = {We present amodel to estimatemotion frommonocular visual and inertialmeasurements. We analyze the model and characterize the conditions under which its state is observable, and its parameters are identifiable. These include the unknown gravity vector, and the unknown transformation between the camera coordinate frame and the inertial unit. We show that it is possible to estimate both state and parameters as part of an on-line procedure, but only provided that the motion sequence is “rich enough,” a condition that we characterize explicitly. We then describe an efficient implementation of a filter to estimate the state and parameters of this model, including gravity and camera-to-inertial calibration. It runs in real-time on an embedded platform, and its performance has been tested extensively. We report experiments of continuous operation, without failures, re-initialization, or re-calibration, on paths of length up to 30Km. We also describe an integrated approach to “loop-closure,” that is the recognition of previously-seen locations and the topological re-adjustment of the traveled path. It represents visual features relative to the global orientation reference provided by the gravity vector estimated by the filter, and relative to the scale provided by their known position within the map; these features are organized into “locations” defined by visibility constraints, represented in a topological graph, where loop closure can be performed without the need to re-compute past trajectories or perform bundle adjustment. The software infrastructure as well as the embedded platform is described in detail in a technical report (Jones and Soatto (2009).)},
author = {Jones, E. S. and Soatto, S.},
doi = {10.1177/0278364910388963},
file = {:home/jkaiser/Downloads/jonesS10IJRR.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
pages = {1--38},
title = {{Visual-inertial navigation, mapping and localization: A scalable real-time causal approach}},
year = {2011}
}
@article{Klein2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
author = {Klein, Georg and Murray, David},
doi = {10.1109/ISMAR.2007.4538852},
file = {:home/jkaiser/Downloads/KleinMurray2007ISMAR.pdf:pdf},
isbn = {9781424417506},
issn = {00472778},
journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR},
title = {{Parallel tracking and mapping for small AR workspaces}},
year = {2007}
}


@article{Leute2014,
author = {Stefan, Leutenegger and Simon, Lynen and Michael, Bosse and Roland, Siegwart and Paul, Furgale},
journal = {The International Journal of Robotics Research},
number = {3},
pages = {314--334},
title = {{Keyframe-based visual-inertial odometry using nonlinear optimization}},
volume = {34},
year = {2014}
}

@article{LynenIROS13,
title={A robust and modular multi-sensor fusion approach applied to mav navigation},
author={Lynen, Simon and Achtelik, Markus W and Weiss, Steven and Chli, Maria and Siegwart, Roland},
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
pages={3923--3929},
year={2013},
organization={IEEE}
}


@article{Li2013a,
author = {Li, Bo and Heng, Lionel and Lee, Gim Hee and Pollefeys, Marc},
doi = {10.1109/IROS.2013.6696562},
file = {:home/jkaiser/Downloads/LiIROS13b.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1595--1601},
title = {{A 4-point algorithm for relative pose estimation of a calibrated camera with a known relative rotation angle}},
year = {2013}
}
@article{Li2006,
abstract = {Estimating relative camera motion from two calibrated views is a classical problem in computer vision. The minimal case for such problem is the so-called five-point problem, for which the state-of-the-art solution is Nister's algorithm (2003, 2004). However, due to the heuristic nature of the procedures it applies, to implement it needs much effort for non-expert user. This paper provides a simpler algorithm based on the hidden variable resultant technique. Instead of eliminating the unknown variables one by one (i.e, sequentially) using the Gauss elimination, our algorithm eliminates many unknowns at once. Moreover, in the equation solving stage, instead of back-substituting and solve all the unknowns sequentially, we compute the minimal singular vector of the coefficient matrix, by which all the unknown parameters can be estimated simultaneously. Experiments on both simulation and real images have validated the new algorithm},
author = {Li, Hongdong and Hartley, Richard},
doi = {10.1109/ICPR.2006.579},
file = {:home/jkaiser/Downloads/new5pt\_cameraREady\_ver\_1.pdf:pdf},
isbn = {0769525210},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {630--633},
title = {{Five-point motion estimation made easy}},
volume = {1},
year = {2006}
}
@article{Li2013,
author = {Li, M. and Mourikis, a. I.},
doi = {10.1177/0278364913481251},
file = {:home/jkaiser/Downloads/VIO-paper.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {extended kalman filter consistency,vision-aided inertial navigation,visual-inertial odometry,visual-inertial slam},
number = {6},
pages = {690--711},
title = {{High-precision, consistent EKF-based visual-inertial odometry}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364913481251},
volume = {32},
year = {2013}
}
@misc{Longuet-Higgins1981,
abstract = {A simple algorithm for computing the three-dimensional structure of a scene from a correlated pair of perspective projections is described here, when the spatial relationship between the two projections is unknown. This problem is relevant not only to photographic surveying1 but also to binocular vision2, where the non-visual information available to the observer about the orientation and focal length of each eye is much less accurate than the optical information supplied by the retinal images themselves. The problem also arises in monocular perception of motion3, where the two projections represent views which are separated in time as well as space. As Marr and Poggio4 have noted, the fusing of two images to produce a three-dimensional percept involves two distinct processes: the establishment of a 1:1 correspondence between image points in the two views—the ‘correspondence problem’—and the use of the associated disparities for determining the distances of visible elements in the scene. I shall assume that the correspondence problem has been solved; the problem of reconstructing the scene then reduces to that of finding the relative orientation of the two viewpoints.},
author = {Longuet-Higgins, H. C.},
booktitle = {Nature},
doi = {10.1038/293133a0},
file = {:home/jkaiser/Downloads/sceneRec2.pdf:pdf},
isbn = {8880551515},
issn = {0028-0836},
number = {5828},
pages = {133--135},
title = {{A computer algorithm for reconstructing a scene from two projections}},
volume = {293},
year = {1981}
}
@article{Lupton2012,
abstract = {In this paper, we present a novel method to fuse observations from an inertial measurement unit (IMU) and visual sensors, such that initial conditions of the inertial integration, including gravity estimation, can be recovered quickly and in a linear manner, thus removing any need for special initialization procedures. The algorithm is implemented using a graphical simultaneous localization and mapping like approach that guarantees constant time output. This paper discusses the technical aspects of the work, including observability and the ability for the system to estimate scale in real time. Results are presented of the system, estimating the platforms position, velocity, and attitude, as well as gravity vector and sensor alignment and calibration on-line in a built environment. This paper discusses the system setup, describing the real-time integration of the IMU data with either stereo or monocular vision data. We focus on human motion for the purposes of emulating high-dynamic motion, as well as to provide a localization system for future human-robot interaction.},
author = {Lupton, Todd and Sukkarieh, Salah},
doi = {10.1109/TRO.2011.2170332},
file = {:home/jkaiser/Downloads/lupton.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Field robots,localization,search-and-rescue robots,sensor fusion},
number = {1},
pages = {61--76},
title = {{Visual-inertial-aided navigation for high-dynamic motion in built environments without initial conditions}},
volume = {28},
year = {2012}
}
@article{Mahony2008,
abstract = {This paper considers the problem of obtaining good attitude estimates from measurements obtained from typical low cost inertial measurement units. The outputs of such systems are characterized by high noise levels and time varying additive biases. We formulate the filtering problem as deterministic observer kinematics posed directly on the special orthogonal group SO (3) driven by reconstructed attitude and angular velocity measurements. Lyapunov analysis results for the proposed observers are derived that ensure almost global stability of the observer error. The approach taken leads to an observer that we term the direct complementary filter. By exploiting the geometry of the special orthogonal group a related observer, termed the passive complementary filter, is derived that decouples the gyro measurements from the reconstructed attitude in the observer inputs. Both the direct and passive filters can be extended to estimate gyro bias online. The passive filter is further developed to provide a formulation in terms of the measurement error that avoids any algebraic reconstruction of the attitude. This leads to an observer on SO(3), termed the explicit complementary filter, that requires only accelerometer and gyro outputs; is suitable for implementation on embedded hardware; and provides good attitude estimates as well as estimating the gyro biases online. The performance of the observers are demonstrated with a set of experiments performed on a robotic test-bed and a radio controlled unmanned aerial vehicle.},
author = {Mahony, Robert and Hamel, Tarek and Pflimlin, Jean Michel},
doi = {10.1109/TAC.2008.923738},
file = {:home/jkaiser/Downloads/nonlinearComplementaryFilter.pdf:pdf},
isbn = {0780395689},
issn = {00189286},
journal = {IEEE Transactions on Automatic Control},
keywords = {Attitude estimates,Complementary filter,Nonlinear observer,Special orthogonal group},
number = {5},
pages = {1203--1218},
title = {{Nonlinear complementary filters on the special orthogonal group}},
volume = {53},
year = {2008}
}

@article{Majdik2015,
author = {A. L. Majdik, D. Verda, Y Albers-Schoenberg and D. Scaramuzza},
doi = {10.1002/rob.21585},
journal = {Journal of Field Robotics},
pages = {1--25},
title = {{Air-ground Matching: Appearance-based GPS-denied
Urban Localization of Micro Aerial Vehicles}},
year = {2015}
}


@article{Martinelli2014,
author = {Martinelli, Agostino},
doi = {10.1007/s11263-013-0647-7},
file = {:home/jkaiser/Downloads/10.1007\_s11263-013-0647-7.pdf:pdf},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Inertial sensors,Robotics,Sensor fusion,Structure from motion},
number = {2},
pages = {138--152},
title = {{Closed-form solution of visual-inertial structure from motion}},
volume = {106},
year = {2014}
}
@article{Martinelli2012,
  title={Vision and IMU data fusion: Closed-form solutions for attitude, speed, absolute scale, and bias determination},
  author={Martinelli, Agostino},
  journal={Transactions on Robotics},
  volume={28},
  number={1},
  pages={44--60},
  year={2012},
  publisher={IEEE}
}
@article{Metni2005,
abstract = { In this paper, a nonlinear complimentary filter (x-estimator) is presented to estimate the attitude of a UAV (unmanned aerial vehicle). The measurements are taken from a low-cost SMU (inertial measurement unit) which consists of 3-axis accelerometers and 3-axis gyroscopes. The gyro bias are estimated online. A second nonlinear complimentary filter (z-estimator) is also designed, it combines 3-axis gyroscope readings with 3-axis magnetometer measurements. From the proposed estimators, the full rotation matrix R will be retrieved. Both estimators use the fact that the orientation matrix, evolving on SO(3), is not locally parameterized and thus could be used to describe any kind of 3D motion. Convergence of the two observers is theoretically proved and simulations as well as experiments are conducted on a real platform in hovering flight conditions.},
author = {Metni, Najib and Pflimlin, Jean Michel and Hamel, Tarek and Sou\`{e}res, Philippe},
doi = {10.1109/IROS.2005.1544997},
file = {:home/jkaiser/Downloads/biasEstimation.pdf:pdf},
isbn = {0780389123},
issn = {09670661},
journal = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
keywords = {Aerial robots,Inspection robots,Mobile robot navigation,Sensor fusion},
pages = {295--301},
title = {{Attitude and gyro bias estimation for a flying UAV}},
year = {2005}
}
@article{Nister2003,
author = {Nister, D},
file = {:home/jkaiser/Downloads/5pt.pdf:pdf},
pages = {195--202},
title = {{An efficient solution to the five point relative pose problem}},
year = {2003}
}
@article{Rosten2005,
abstract = {This paper addresses the problem of real-time 3D model-based tracking by combining point-based and edge-based tracking systems. We present a careful analysis of the properties of these two sensor systems and show that this leads to some non -trivial design choices that collectively yield extremely high performance. In particular, we present a method for integrating the two systems and robustly combining the pose estimates they produce. Further we show how on-line learning can be used to improve the performance of feature tracking. Finally, to aid real-time performance, we introduce the FAST feature detector which can perform full-frame feature detection at 400Hz. The combination of these techniques results in a system which is capable of tracking average prediction errors of 200 pixels. This level of robustness allows us to track very rapid motions, such as 50deg camera shake at 6Hz},
author = {Rosten, Edward and Drummond, Tom},
doi = {10.1109/ICCV.2005.104},
file = {:home/jkaiser/Downloads/rosten\_2005\_tracking.pdf:pdf},
isbn = {076952334X},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1508--1515},
title = {{Fusing points and lines for high performance tracking}},
volume = {II},
year = {2005}
}
@article{Rosten2006,
abstract = {Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7\% of the available processing time. By comparison neither the Harris detector (120\%) nor the detection stage of SIFT (300\%) can operate at full frame rate. Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.},
author = {Rosten, Edward and Drummond, Tom},
doi = {10.1007/11744023\_34},
file = {:home/jkaiser/Downloads/rosten\_2006\_machine.pdf:pdf},
isbn = {3540338322},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {430--443},
pmid = {18684738},
title = {{Machine learning for high-speed corner detection}},
volume = {3951 LNCS},
year = {2006}
}
@article{Sabatini2011,
abstract = {In this paper we present a quaternion-based Extended Kalman Filter (EKF) for estimating the three-dimensional orientation of a rigid body. The EKF exploits the measurements from an Inertial Measurement Unit (IMU) that is integrated with a tri-axial magnetic sensor. Magnetic disturbances and gyro bias errors are modeled and compensated by including them in the filter state vector. We employ the observability rank criterion based on Lie derivatives to verify the conditions under which the nonlinear system that describes the process of motion tracking by the IMU is observable, namely it may provide sufficient information for performing the estimation task with bounded estimation errors. The observability conditions are that the magnetic field, perturbed by first-order Gauss-Markov magnetic variations, and the gravity vector are not collinear and that the IMU is subject to some angular motions. Computer simulations and experimental testing are presented to evaluate the algorithm performance, including when the observability conditions are critical.},
author = {Sabatini, Angelo Maria},
doi = {10.3390/s111009182},
file = {:home/jkaiser/Downloads/sensors-11-09182.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Ambulatory human motion tracking,Extended kalman filter,Inertial measurement unit,Lie derivatives,Observability of nonlinear systems,Orientation determination},
number = {10},
pages = {9182--9206},
pmid = {22163689},
title = {{Kalman-filter-based orientation determination using inertial/magnetic sensors: Observability analysis and performance evaluation}},
volume = {11},
year = {2011}
}
@article{Schops2014,
author = {Sch\"{o}ps, T and Engel, J and Cremers, D},
doi = {10.1109/ISMAR.2014.6948420},
file = {:home/jkaiser/Downloads/schoeps14ismar.pdf:pdf},
isbn = {978-1-4799-6184-9},
journal = {IEEE International Symposium on Mixed and Augmented Reality},
keywords = {3d reconstruction,ar,direct visual odometry,map-,mobile devices,neon,ping,semi-dense,tracking},
pages = {1--6},
title = {{Semi-dense visual odometry for AR on a smartphone}},
year = {2014}
}
@article{Tereshkov2013,
abstract = {A simple approach to gyro and accelerometer bias estimation is proposed. It does not involve Kalman filtering or similar formal techniques. Instead, it decouples the estimation problem into two separate stages. At the first stage, inertial system attitude errors are damped by means of a feedback from an external aid. In the presence of uncompensated biases, the steady-state feedback rebalances those biases and can be used to estimate them. At the second stage, the desired bias estimates are expressed in a closed form in terms of the damping feedback. The estimator has only three tunable parameters and is easy to implement and use. The tests proved the feasibility of the proposed approach for the estimation of low-cost MEMS inertial sensor biases on a moving vehicle.},
archivePrefix = {arXiv},
arxivId = {1212.0892},
author = {Tereshkov, Vasiliy M.},
doi = {10.1155/2013/762758},
eprint = {1212.0892},
file = {:home/jkaiser/Downloads/gyroBiasEstimation.pdf:pdf},
issn = {16875990},
journal = {International Journal of Navigation and Observation},
keywords = {inertial navigation,kalman filters,sensor systems,state estimation},
pages = {1--6},
title = {{An intuitive approach to inertial sensor bias estimation}},
year = {2013}
}
@article{Veth2007,
abstract = {Navigation parameters (position, velocity, and attitude) can be estimated$\backslash$nusing optical measurements combined with an inertial navigation system.$\backslash$nThis can be accomplished by tracking stationary optical features$\backslash$nin multiple images and using the resulting geometry to estimate and$\backslash$nremove inertial errors. A critical factor governing the performance$\backslash$nof image-aided inertial navigation systems is the robustness of the$\backslash$nfeature tracking algorithm. Previous research has shown the benefit$\backslash$nof coupling the sensors at the measurement level using a tactical-grade$\backslash$ninertial sensor. While the tactical-grade sensor is a reasonable$\backslash$nchoice for larger platforms, the greater size and cost of the sensor$\backslash$nlimits its use in smaller platforms. In this paper, an image-aided$\backslash$ninertial navigation algorithm is implemented using a multidimensional$\backslash$nstochastic feature tracker and low-cost sensors. The performance$\backslash$nof the resulting navigation system is evaluated and compared. The$\backslash$nfused image-inertial sensor is shown to outperform a free-running$\backslash$ntactical-grade inertial sensor.},
author = {Veth, M. and Raquet, J.},
file = {:home/jkaiser/Downloads/ADA462964.pdf:pdf},
issn = {00281522},
journal = {J. Inst. Navigat.},
number = {1},
pages = {11--20},
title = {{Fusing low-cost image and inertial sensors for passive navigation .pdf}},
volume = {54},
year = {2007}
}
@article{Weiss2012,
abstract = {Introduccion Se comienza por hablar de la evolucion del sistema de vision animal al humano en una analogia con las camaras mas recientes. Luego se habla de animales que usan otros sensores como los murcielagos (sonar) o algunas aves (campo magnetico) para orientarse. En este caso se hace mencion a los casos de sistemas en estado no observable los cuales pueden ser superados mediante fusion de sensorial. El problema de la posicion cero es de gran dificultad en un UAV dado que sus actuadores no puenden estar en cero. El hecho de estimar en forma precisa el estado para el control del robot motiva el desarrollo de este trabajo. En este caso se hace uso de metodos basados en vision para identificar apropiadamente la fusion de sensores para estimacion de estados y autocalibracion de sensores. Dentro de la motivacion se cuentan 3 aspectos a) el peso de los dispositivos a cargar por un MAV es una resctriccion importante, b) el costo computacional de los calculos debe ser el minimo es decir se debe contar con algoritmos eficientes ya que la disponibilidad energetica es muy limitada, c) un sensor no siempre provee la mejor informacion. Por tal razon lo mejor es contar con una adecuada combinacion de informacion sensorial para la estimacion de los estados.},
author = {Weiss, Stephan M},
doi = {10.3929/ethz-a-007344020},
file = {:home/jkaiser/Downloads/eth-5889-02.pdf:pdf},
keywords = {monocular SLAM,multi sensor fusion,vision based MAV navigation},
number = {20305},
title = {{Vision Based Navigation for Micro Helicopters (PhD Thesis - Weiss 2012)}},
year = {2012}
}
@article{Yadlin2008,
abstract = {The US Air Force Academy has an advanced and largely self-sustaining small satellite program. Cadets are responsible for the design, testing, and construction of a series of small satellites. In anticipation of this program, the Astronautics Department selects cadets to participate in research internships. Research in one such internship focused on using low cost attitude sensors and gyroscopes to estimate the attitude of a satellite and determine the gyroscopes’ bias. Kalman filtering was used to determine this bias and estimate the attitude in three axes, using measurements in only two axes. A computerized satellite dynamic model was used to test the bias estimation filter. Next, the code was tested using actual data collected on an air-bearing table at the University of Surrey’s Surrey Space Centre. Finally, the code was connected directly to the air-bearing table to implement hardware-in-loop testing. This paper details the theory behind the estimator in addition to the implementation of the estimator into the hardware as well as the results of the hardware-in-loop testing. =},
author = {Yadlin, Roni},
file = {:home/jkaiser/Downloads/Attitude Determination and Bias Estimation Using Kalman Filtering - Yadlin.pdf:pdf},
pages = {1--11},
title = {{Attitude Determination and Bias Estimation Using Kalman Filtering}},
year = {2008}
}
@article{Yun2007,
abstract = {Numerous applications require a self-contained personal navigation system that works in indoor and outdoor environments, does not require any infrastructure support, and is not susceptible to jamming. Posture tracking with an array of inertial/magnetic sensors attached to individual human limb segments has been successfully demonstrated. The "sourceless" nature of this technique makes possible full body posture tracking in an area of unlimited size with no supporting infrastructure. Such sensor modules contain three orthogonally mounted angular rate sensors, three orthogonal linear accelerometers and three orthogonal magnetometers. This paper describes a method for using accelerometer data combined with orientation estimates from the same modules to calculate position during walking and running. The periodic nature of these motions includes short periods of zero foot velocity when the foot is in contact with the ground. This pattern allows for precise drift error correction. Relative position is calculated through double integration of drift corrected accelerometer data. Preliminary experimental results for various types of motion including walking, side stepping, and running document accuracy of distance and position estimates.},
author = {Yun, Xiaoping and Bachmann, Eric R and Iv, Hyatt Moore and Calusdian, James and Xiaoping, Yun and Moore, H},
doi = {10.1109/robot.2007.363845},
file = {:home/jkaiser/Downloads/imuPosTracking.pdf:pdf},
isbn = {1050-4729},
issn = {1050-4729},
journal = {Robotics and Automation, 2007 IEEE International Conference on},
keywords = {accelerometers,gait analysis,human movement,inertial sensor,magnetic sensor,magnetic sensors,magnetometers,navigation,optical tracking,orthogonal linear accelerometers,orthogonal magnetometers,posture tracking,selfcontained personal navigation system,selfcontained position tracking},
number = {April},
pages = {2526--2533},
title = {{Self-contained Position Tracking of Human Movement Using Small Inertial/Magnetic Sensor Modules}},
year = {2007}
}
@Book{Hartley2004,
    author = "Hartley, R.~I. and Zisserman, A.",
    title = "Multiple View Geometry in Computer Vision",
    edition = "Second",
    year = "2004",
    publisher = "Cambridge University Press, ISBN: 0521540518"
}
@inproceedings{Forster2014,
  author = {Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
  title = {{SVO}: Fast Semi-Direct Monocular Visual Odometry},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2014}
}

@article{FaesslerICRA15,
title={Autonomous, Vision-based Flight and Live Dense 3D Mapping with a Quadrotor Micro Aerial Vehicle},
author={Faessler, Matthias and Fontana, Flavio and Forster, Christian and Mueggler, Elias and Pizzoli, Matia and Scaramuzza, Davide},
journal={Journal of Field Robotics},
year={2015},
publisher={Wiley Online Library}
}