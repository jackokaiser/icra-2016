%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

%% \IEEEoverridecommandlockouts                              % This command is only needed if
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage [vscale=0.76,includehead]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{fullpage}
\usepackage{mathptmx} % font = times
\usepackage{helvet} % font sf = helvetica
\usepackage[latin1]{inputenc}
\usepackage{relsize}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{pgfplots}

% argument #1: any options
\newenvironment{customlegend}[1][]{%
    \begingroup
    % inits/clears the lists (which might be populated from previous
    % axes):
    \csname pgfplots@init@cleared@structures\endcsname
    \pgfplotsset{#1}%
}{%
    % draws the legend:
    \csname pgfplots@createlegend\endcsname
    \endgroup
}%

% makes \addlegendimage available (typically only available within an
% axis environment):
\def\addlegendimage{\csname pgfplots@addlegendimage\endcsname}
\def\addlegendentry{\csname pgfplots@addlegendentry\endcsname}


\newcommand{\rot}[2]{\ensuremath{C_{\,#1}^{\,#2}}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\definecolor{amethyst}{rgb}{0.5, 0.3, 0.7}

% and optionally (as of Pgfplots 1.3):
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}

\title{\LARGE \bf
Absolute scale velocity determination combining visual and inertial measurements for micro aerial vehicles
}


\author{Jacques Kaiser and Agostino Martinelli% <-this % stops a space
%% \thanks{*This work was not supported by any organization}% <-this % stops a space
%% \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%%         University of Twente, 7500 AE Enschede, The Netherlands
%%         {\tt\small albert.author@papercept.net}}%
%% \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
%%         Dayton, OH 45435, USA
%%         {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  State of the art approaches for visual-inertial sensor fusion are filter based algorithms.
  These methods are recursive by design and therefore require an initialization.
  Due to the nonlinearity of these systems, a poor initialization can have a dramatic impact on the performance of the estimations.
  Recently a closed-form solution providing such an initialization has been derived.
  Despite mathematically sound, it is not robust to noisy sensor data in practice.
  In this paper, we study the impact of noisy sensor on the performance of the method.
  Specifically, the gyroscope bias is shown to be a major performance bottleneck.
  We then propose a method to automatically estimate this bias.
  Compared to the original method, the new method is now robust to this bias, and also provides the gyroscope bias.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{INTRODUCTION}




Autonomous mobile robots navigating in unknown environments have an intrinsic need to perform localization and mapping using only on-board sensors.
Concerning Micro Aerial Vehicles (MAV), a critical issue is to limit the number of on-board sensors to reduce weight and power consumption.
Therefore, a common setup is to combine a monocular camera with an inertial measurements unit (IMU).
On top of being cheap, these sensors have very interesting complementarities.
Additionally, they can operate in indoor environments where Global Positioning System (GPS) signals are shadowed.
An open question is how to optimally fuse the information provided by these sensors.

Currently, most sensor fusion algorithms are either filter based or iterative. That is, given a current state and measurements, they return an updated state.
While working well in practice, these algorithms need to be provided by an external initial state.

The initialization of a filter based method is critical.
Due to nonlinearities of the system, a poor initialization can result into converging towards local minima and  providing faulty states with high confidence.
%% Indeed, another shortcoming of filters is that they can silently fail.

In this paper we demonstrate the efficiency of a recent closed-form solution introduced in \cite{Martinelli2012}\cite{Martinelli2014} that fuses visual and inertial data to obtain the structure of the environment at the global scale along with the attitude and the speed of the robot.
By nature, a closed-form solution is deterministic and thus does not require any initialization.

%% It is assumed that the camera is calibrated and the transformation between the IMU and the camera is known.
%% This is a fair assumption for industrial drones to come pre-calibrated.

We implemented this method in order to test it with real terrain data.
This allowed us to identify its bottlenecks and bring modifications to overcome them.
Specifically, we investigated the impact of biased inertial measurements.
Despite the case of biased accelerometer was originally studied in \cite{Martinelli2014} we show that its low impact on the system makes it hard to estimate.

%% Specifically, we started by reformulating the equations for greater numerical stability.
%% This lead to a major leap in the quality of the estimations.

One major bottleneck of this method was the impact of biased gyroscope measurements.
In other words, the performance becomes very poor in presence of a bias on the gyroscope and, in practice, the overall method could only be successfully used with a very precise - and expensive - gyroscope.
We then introduced a simple method that automatically estimates this bias.

%% However, this method requires a significant amount of data to provide correct estimates of the gyroscope bias (either many features or long time of integration).
%% We were able to get rid of this limitation with a simple statistical trick by adding a regularization term to our cost function.
%tweak in the cost function we are minimizing.

By adding this new method for the bias estimation to the original method we obtain results which are equivalent to the ones in absence of bias.
Compared to the original method, the new method is now robust to the gyroscope bias, and also provides the gyroscope bias.
%% Specifically, this method can accurately estimate the initial speed, the gravity, the distance to the features and the gyroscope bias for very short time of integration (around 2 seconds) and low amount of observed point features (around 5).


%% \begin{figure}[h!]
%%         \centering
%%    \resizebox{0.4\textwidth}{!}{\input{./graph}}
%%         \caption{Estimation error of the original formulation of the Closed-Form solution against the improved Closed-Form solution observing 7 features over 3 seconds.}
%% \end{figure}

\section{RELATED WORK}

The problem of fusing vision and inertial data has been extensively investigated in the past.
However, most of the proposed methods require an external state initialization.
Because of the system nonlinearities, lack of precise initialization can irreparably damage the entire estimation process.
In literature, this initialization is often guessed or assumed to be known \cite{Armesto2007}\cite{Li2013}\cite{Huang2009}\cite{Bibuli2007}\cite{Forster2014}.

We are therefore interested into a deterministic solution that analytically expresses the state in terms of the measurements provided by the sensors during a short time-interval.

Some deterministic solutions have been introduced in the field of computer vision and rely only on visual measurements.
These techniques can recover the relative rotation and translation up to scale between two camera poses \cite{Longuet-Higgins1981}\cite{Hartley1997}\cite{Nister2003}\cite{Hartley2004}\cite{Li2006}.
These techniques are currently used in state-of-the-art visual navigation methods on MAV in order to initialize maps \cite{Weiss2012}\cite{Forster2014}.
However, the knowledge of the absolute scale and at least the absolute roll and pitch angles are essential for the MAV.
It is required to take the inertial measurements into consideration to compute these values deterministically.

%% Some visual inertial sensor fusion that works without initialization have been introduced.
%% In \cite{Lupton2012} the authors pre-integrate their IMU data in an arbitrary frame moving at the same velocity than the previous body frame of the MAV.
%% Since the reference frame is moving at the same instantaneous velocity as the vehicle was at the previous pose, the initial velocity with respect to this frame is zero, making integration without initial conditions possible.
%% The camera measurements are then used to match the initial reference frame with the defined reference frame.
%% Their method can recover the absolute scale, roll and pitch angles, in a linear manner.
%% However, their method is purely numerical and we can not analytically derive its properties.

%, therefore no assumptions can be made about its properties.


A closed-form solution is provided in \cite{Engel2013} to determine the absolute scale.
However they assume an additional on-board sensor measuring a metric quantity, such as an altimeter or an air pressure sensor.
It is therefore not applicable for our minimal configuration.

A procedure to quickly re-initialize a MAV after a failure is discussed in \cite{Faessler2015}.
However, their method also requires an altimeter to estimate the absolute scale.

A landmark of known dimension is used in \cite{Gemeiner2007} to recover the initial pose of the MAV.
This method is therefore not suited to unknown environment.

Recently, a closed-form solution has been introduced in \cite{Martinelli2012}.
From integrating inertial and visual measurements over a short time-interval, this solution provides the absolute scale, roll and pitch angles, initial velocity and distance to features.
Specifically, all the physical quantities are obtained by simply inverting a linear system.
The solution of the linear system can be refined with a quadratic equation assuming the knowledge of the gravity magnitude.

This closed-form has been improved in \cite{Li2013} to work with unknown camera-IMU calibration.
Specifically, the translation between the camera and the IMU is expressed as an unknown in the linear system.
However, the relative rotation between the camera and the IMU can not be expressed this way,
thus an alternative way to compute it is proposed.
Their method is therefore independent of external camera-IMU calibration, hence well suited for power-on-and-go systems.

A more intuitive expression of the same closed-form solution is derived in \cite{Martinelli2014}.
This formulation also provides the accelerometer bias, which can also be expressed as an unknown in the linear system.

While being mathematically sound, this closed-form solution is not robust to noisy sensor data \cite{Faessler2015}.
For this reason, to the best of our knowledge, it has never been used in an actual application.
In this paper we carry out an analysis to find out its limitations.
Specifically, we show that it is resilient to the accelerometer bias but strongly affected by the gyroscope bias.
We then introduce a simple method that automatically estimates the gyroscope bias.
By adding this new method for the bias estimation to the original method we obtain results which are equivalent to the ones in absence of bias.
Compared to the original method, the new method is now robust to the gyroscope bias, and also provides the gyroscope bias.
We validate our new method against real terrain data to prove its robustness against noisy sensors.

\section{THE CLOSED-FORM SOLUTION}

In this paper, we do not provide a new derivation of the closed-form solution.
Instead, we consider the latest derivation proposed in \cite{Martinelli2014}.
Specifically, the author expresses the state of the MAV with respect to the visual and inertial measurements in Equation \ref{eq:final1}:


\begin{equation} \tag{6} \label{eq:final1}
S_j = \lambda_1^i\mu_1^i - V t_j - G \frac{t_j^2}{2} - \lambda^i_j \mu^i_j
\end{equation}
With:
\begin{itemize}
\item $\mu_j^i$ the normalized bearing of point feature $i$ at time $t_j$ in the initial local frame;
\item $\lambda_j^i$ the distance to the point feature $i$ at time $t_j$;
\item $V$ the initial velocity in the initial local frame;
\item $G$ the initial gravity in the initial local frame;
\item $S_j$ the integration up to time $t_j$ of the rotated linear acceleration data.
\end{itemize}


The local frame refers to a frame of reference common to the IMU and the camera.
In a real application, we would work in the IMU frame and have some additional constant terms
accounting for the camera-IMU transformation.
We do not express these constant calibration terms explicitly here for clarity reasons.

The unknowns of Equation \ref{eq:final1} are the scalars $\lambda_j^i$ and the vectors $V$ and $G$.
Note that the knowledge of $G$ is equivalent to the knowledge of the roll and pitch angles.
The vectors $\mu_j^i$ are fully determined by visual and gyroscope measurements,
and the vectors $S_j$ are determined by accelerometer and gyroscope measurements.

\begin{figure}
  \centering
  \includegraphics[width=0.90\columnwidth]{images/closedFormExplained}
  \caption{Visual representation of Equation \ref{eq:final1}.
  The unknowns of the equation are colored in \textcolor{amethyst}{purple}.}
\end{figure}

Equation \ref{eq:final1} holds for each three dimensions of all point features $i=1,...,N$ and each frame starting from the second one $j=2,...n_i$.
We therefore have a linear system consisting of $3(n_i-1)N$ equations in $6 + Nn_i$ unknowns.
Indeed, note that when the first frame occurs, at $t_j = 0$,
Equation \ref{eq:final1} is always satisfied thus does not provide information.
We can write our system using matrix formulation.
Solving the system is equivalent to inverting a matrix of $3(n_i-1)N$ rows and $6+Nn_i$ columns.

In \cite{Martinelli2014}, the author proceeded to one more step before expressing the underlying linear system.
For a given frame $j$, the equation of the first point feature $i=1$ is subtracted to all other point features equation $1<i<=N$ concerning frame $j$ (Equation 7).
This additional step has the effect to corrupt all measurements with the first measurement,
hence worsening the performance of the closed-form solution.
In this paper, we do not take to this additional step.

The linear system in Equation \ref{eq:final1} can be written in the following compact format:

\begin{equation}
\label{eq:mat1} \tag{9}
\Xi X = S
\end{equation}

The matrix $\Xi$ and the vector $S$ are fully determined by the measurements, while $X$ is the unknown vector.
We have:

\[
S \equiv [S_2^T, ...,S_2^T, S_3^T,...,S_3^T,...,S_{n_i}^T,...,S_{n_i}^T]^T
\]
\[
X \equiv [ G^T, V^T, \lambda_1^1, ..., \lambda_1^N, ..., \lambda_{n_i}^1, ..., \lambda_{n_i}^N]^T \\
\]

{
\arraycolsep=3pt % default: 5pt
\medmuskip = 1mu % default: 4mu plus 2mu minus 4mu
\begin{multline*}
  \Xi \equiv \\
      {
        \footnotesize
        \left[
          \begin{array}{l|l|l|l|l|l|l|l|l|l|l}
            T_2 & S_2 & \mu_1^1 & 0_3 & 0_3 & -\mu_2^1 & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 \\
            T_2 & S_2 & 0_3 & \mu_1^2 & 0_3 & 0_3 & -\mu_2^2 & 0_3 & 0_3 & 0_3 & 0_3 \\
            ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
            T_2 & S_2 & 0_3 & 0_3 & \mu_1^N & 0_3 & 0_3 & -\mu_2^N & 0_3 & 0_3 & 0_3 \\
            ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
            ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
            T_{n_i} & S_{n_i} & \mu_1^1 & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 & -\mu_{n_i}^1 & 0_3 & 0_3 \\
            T_{n_i} & S_{n_i} & 0_3 & \mu_1^2 & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 & -\mu_{n_i}^2 & 0_3 \\
            ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
            T_{n_i} & S_{n_i} & 0_3 & 0_3 & \mu_1^N & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 & -\mu_{n_i}^N
          \end{array}
          \right]
      }
\end{multline*}
}

Where $T_j \equiv - \frac{t^2_j}{2} I_3$, $S_j \equiv -t_j I_3$ and $I_3$ is the identity 3 x 3 matrix; $0_{33}$ is the 3 x 3 zero matrix.
Note that the matrix $\Xi$ and the vector $S$ are slightly different from the one proposed in \cite{Martinelli2014}.
This is due to the additional step we did not take for numerical stability reasons.

The sensor information is completely contained in the above linear system. Additionally, in \cite{Martinelli2014}, the author added a quadratic equation assuming the gravitational acceleration is a priori known.
Let us denote the gravitational magnitude by $g$.
We have the extra constraint $|G| = g$. We can express this constraint in matrix formulation:

\begin{equation}
\label{eq:mat2} \tag{10}
| \Pi X | ^2 = g^2
\end{equation}

With $\Pi \equiv [I_3, 0_3, ..., 0_3]$.

We can therefore recover the initial velocity, the roll and pitch angles and the distances to the point features
by finding the vector $X$ which satisfies \ref{eq:mat1} and \ref{eq:mat2}.

In the next sections, we will evaluate the performance of this method on real terrain data.
This will allow us to identify its weaknesses and bring modifications to overcome them.

%% The system therefore becomes:

%% \begin{equation} \label{eq:final2}
%% \left[
%% \begin{array}{lcl}
%% S_j &=& \lambda_1^i\mu_1^i - V t_j - G \frac{t_j^2}{2} - \lambda^i_j \mu^i_j\\
%% 0_3 &=& \lambda_1^1\mu_1^1 - \lambda_j^1\mu_j^1 - \lambda_1^i\mu_1^i + \lambda^i_j \mu^i_j
%% \end{array}
%% \right.
%% \end{equation}



\section{PERFORMANCE BOTTLENECKS}

\subsection{Test setup}

The MAV performs a motion while being tracked with an optical Vicon system.
We can therefore compare our estimations with the ground truth.
We define the relative error as the euclidean distance between the estimation and the ground truth,
normalized by the ground truth.
We measure our error on the absolute scale by computing the mean error over all estimated distances to point features $\lambda_j^i$.

To identify the performance bottlenecks, we used IMU data obtained from terrain acquisitions
while we simulated the visual measurements.
This separation allowed us to know the ground truth for the distance to the point features
and also better understand the weaknesses of our method.

We represent this setup in Fig. \ref{fig:testsetup}.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{images/setupTestDroneError.png}
  \caption{Test setup for identifying the performance bottlenecks.
    The drone is equipped with an IMU, and the visual measurements are simulated.
    It performs a motion while being tracked by a Vicon system.\label{fig:testsetup}}
\end{figure}

In general, we use one frame every 0.1 seconds, even if the camera provides significantly more frames.
Indeed, we can discard most of the frames provided by the camera.
If two frames are too close to each other, then the additional equations do not bring much information to our system.
Reducing the number of considered frames reduces the size of the matrices, thus speeds up the computations.

As an example, over a time interval of 3 seconds, we obtain $31$ distinct frames.
When observing 7 features, it yields a system of $3\times 30\times 7 = 630$ equations and $6+7\times 31=223$ unknowns.

The method we use to solve the overconstrained linear system $\Xi X = S$ is a singular value decomposition (SVD) since it yields numerically robust solutions.

In this section, we will start by presenting the results obtained with the original closed-form solution on terrain IMU data.
Our goal is to identify its performance bottlenecks and introduce modifications to overcome them.

%% The case of a biased gyroscope is addressed in chapter \ref{ch:gyro}.
%% Indeed, the main contribution of this report is the simple method introduced in Section \ref{sec:computeBias} to compute the gyroscope bias with the closed-form solution, which happened to be a major performance bottleneck.

\subsection{Original closed-form solution performance}

The original closed-form solution described in Equation \ref{eq:mat1} will be used as a basis for our work.
Moreover, we can also use the knowledge of the gravity magnitude in order to refine our results with Equation \ref{eq:mat2}.
In this case, we are minimizing a linear objective function with a quadratic constraint.
in Fig. \ref{fig:original}, we represent the quality of the evaluations with and without this additional constraint.


\begin{figure}
  \centering
    \resizebox{0.6\columnwidth}{!}{\input{graph/droneMotion14.6for5s}}
    \caption{Motion performed by the drone in 5 seconds.}
\end{figure}

\begin{figure}
  \centering
    \resizebox{0.7\columnwidth}{!}{\input{graph/originalCF}}
    \caption{Original closed-form solution estimations with and without gravity knowledge refinement. We are observing 5 features  over a variable duration of integration.\label{fig:original}}
\end{figure}


Note how the evaluations get better as we increase the integration duration.
Indeed, our equations come from an extended triangulation \cite{Martinelli2012}.
Therefore, it requires a significant difference in the measurements over time to robustly estimate the state.
Also note that the gravity is robustly estimated (around 5\% error), whereas the speed and the distance to the features is more erroneous (above 10\% error).

Since the gravity is well estimated with the original closed-form, it comes without surprise that constraining its magnitude does not improve the performance much.
The distance to the features are slightly improved (around 1\% error decrease) at the expense of worsening the estimation of the speed (around 20\% error increase).

%% In the following sections, we will introduce modifications to improve over these results.
In the following sections, we will study the impact of biased inertial measurements on the performance of the closed-form solution without considering the gravity refinement.

\subsection{Impact of accelerometer bias on the performance}
In order to visualize the impact of the accelerometer bias on the performance,
we corrupt the accelerometer measurements provided by our terrain IMU by adding an artificial bias (Fig. \ref{fig:biasAccCF}).

Despite a high accelerometer bias the closed-form solution still provides robust results.

\begin{figure}
  \centering
        \hspace{0.2\columnwidth}%
        \begin{subfigure}[b]{0.3\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasLegend}}
                \caption{Legend}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasGravity}}
                \caption{Gravity estimation error}

        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasSpeed}}
                \caption{Velocity estimation error}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasLambda}}
                \caption{Lambda estimation error}

        \end{subfigure}
        \caption{Impact of the accelerometer bias on the performance of the closed-form solution. We are observing 7 features  over a variable duration of integration.\label{fig:biasAccCF}}
\end{figure}

As seen on the Fig. \ref{fig:biasAccCF}, neither the estimation of the gravity, the velocity or the lambdas are impacted by the accelerometer bias.

In \cite{Martinelli2014}, the author provides an alternative formulation of the closed-form solution including the accelerometer bias as an observable unknown of the system.
However, the estimation of the accelerometer bias with this method is not robust since our system is only slightly affected by it.

This is a counterintuitive results. Since our equations contain an integration of the acceleration, we also perform an integration of the accelerometer bias.
We would have expected the accelerometer bias to have a greater impact on the solutions yielded by the system.



\subsection{Impact of gyroscope bias on the performance}
Again, in order to visualize the impact of the gyroscope bias on the performance,
we corrupt the gyroscope measurements provided by our terrain IMU by adding an artificial bias (Fig. \ref{fig:biasGyroCF}).

Our experiments have shown that the presence of gyroscope bias significantly damages the results of the closed-form solution.

\begin{figure}
  \centering
        \hspace{0.2\columnwidth}%
        \begin{subfigure}[b]{0.3\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasLegend}}
                \caption{Legend}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasGravity}}
                \caption{Gravity estimation error}

        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasSpeed}}
                \caption{Velocity estimation error (from 0\% to 200\%)}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasLambda}}
                \caption{Lambda estimation error}

        \end{subfigure}
        \caption{Impact of the gyroscope bias on the performance of the closed-form solution. We are observing 7 features over a variable duration of integration. \label{fig:biasGyroCF}}
\end{figure}

As seen in Fig. \ref{fig:biasGyroCF}, the performance becomes very poor in presence of a bias on the gyroscope and, in practice, the overall method could only be successfully used with a very precise---and expensive---gyroscope.



\section{ESTIMATING THE GYROSCOPE BIAS}

Previous work has shown that the gyroscope bias is an observable mode when using an IMU and a camera, which means that it can be estimated \cite{Martinelli2012}.

Optimally, we would add the gyroscope bias in our unknown vector $X$ and determine $X$ by simply inverting the system $\Xi X = S$ as in the standard closed-form solution.
However, we can not express the gyroscope bias linearly with this system.

In this section, we propose a different approach to estimate the gyroscope bias using the closed-form solution.

\subsection{Nonlinear minimization of the residual}

Since our system of equations (\ref{eq:final1}) is overconstrained,
inverting it is equivalent to finding the vector $X$ that minimizes the residual $||\Xi X - S||^2$.

Because we can not express the gyroscope bias linearly,
we define the following cost function:

\begin{equation}
  \label{eq:cost}
  cost(B) = ||\Xi X - S||^2
\end{equation}

With:
\begin{itemize}
\item $B$ the gyroscope bias;
\item $\Xi$ and $S$ computed with respect to $B$.
\end{itemize}

By minimizing this cost function, we recover the gyroscope bias $B$ and the unknown vector $X$ which compensated for the gyroscope bias $B$.
We can initialize the optimization process with $B = 0_3$ since the bias is usually a rather small quantity.

\begin{figure}
  \centering
  \resizebox{0.7\columnwidth}{!}{\input{graph/biasEstimation30Ft}}
  \caption{Gyroscope bias estimation from nonlinear minimization of the residual. We are observing 30 features  over a variable duration of integration. The true gyroscope bias is $[0.0276,   -0.0024,    0.0417]$. \label{fig:gBiasEstimate}}
\end{figure}

\begin{figure}
   \centering
   \resizebox{0.7\columnwidth}{!}{\input{graph/CF3vsOpt30Ft}}
                \caption{Estimation error of the closed-form solution against the optimized closed-form solution.  We are observing 30 features  over a variable duration of integration. The true gyroscope bias is $[0.0276,   -0.0024,    0.0417]$. \label{fig:optEstimate}}
\end{figure}

\begin{figure}
        \centering

        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/opt30ftgravity}}
                \caption{Gravity estimation relative error}

        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/opt30ftspeed}}
                \caption{Velocity estimation relative error}

        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/opt30ftLambda}}
                \caption{Lambda estimation relative error}

        \end{subfigure}
        \caption{Impact of the gyroscope bias on the performance of the optimized closed-form solution. We are observing 7 features  over a variable duration of integration. \label{fig:biasGyroOpt}}
\end{figure}

%% This technique is similar to the one used in \cite{Abbeel2005}, where the author minimizes the residual of a Kalman filter in order to recover the noise matrix for the sensors.

The optimized closed-form solution provides better results than the standard closed-form solution. Fig. \ref{fig:optEstimate} depicts an improvement in precision of around $5\%$ for the distance to the features, and around $13\%$ for the speed after 4 seconds of integration.

Moreover, the optimized closed-form solution requires a shorter integration duration to provide good quality results.
Specifically, after 2 seconds of integration (around 7 frames) the provided estimations are already robust.
The non-optimized closed-form solution requires 3 seconds of integration before converging to acceptable estimations.


Lastly, this method is robust even for high values of the gyroscope bias.
Fig. \ref{fig:biasGyroOpt} represents the quality of the estimations with the same artificial gyroscope bias from Fig. \ref{fig:biasGyroCF}.



As seen in Fig. \ref{fig:biasGyroOpt}, after a certain integration duration, the estimations agree no matter how high the bias is.
In other words, given that the integration duration is long enough, this method is unaffected by the gyroscope bias.
%% This method can therefore fully compensate for high values of the gyroscope bias.
%% Moreover, the values on which these evaluations agree are equals to the ones for the curve that has a gyroscope bias set to $0_3$.

However, for very short time of integration ($<2$ seconds), the gyroscope bias can be estimated to large and unlikely values.
To understand this misestimation, in Fig. \ref{fig:cost} we plot the residual with respect to the bias, which is the cost function we are minimizing.
We highlight a misestimation of the gyroscope bias by setting the duration of integration to 2 seconds while observing 7 features.
We refer to the components of the gyroscope bias by $B = [B_x, B_y, B_z]$.

\begin{figure}
        \centering
        \begin{subfigure}[b]{0.47\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/costXZ}}
                \caption{Residual with respect to $B_x$ and $B_z$}
        \end{subfigure}~
        \begin{subfigure}[b]{0.47\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/costYZ}}
                \caption{Residual with respect to $B_y$ and $B_z$}
        \end{subfigure}
        \caption{Cost function (residual) with respect to the gyroscope bias for a small amount of available measurements (integration of 2 seconds while observing 7 features)\label{fig:cost}}
\end{figure}

As we can see in Fig. \ref{fig:cost}, the cost function admits a symmetry with respect to $B_z$.

\subsection{Removing the symmetry in the cost function}

The symmetry in the cost function is induced by the strong weight of the gravity in the equation.
In general, the residual is almost constant with respect to the component of the gyroscope bias along the direction $\vec{u}$ when this direction $\vec{u}$ is collinear with the gravity throughout the motion.
In the terrain data we had, the motion satisfies this constraint.
Specifically, the gyroscope was strapped on the MAV such that the vector $[0,0,1]$ in the gyroscope frame was pointing upwards when the MAV was hovering.
That is why the residual varies only slightly for certain time sequences with respect to this vector.
Indeed, in normal operations, a MAV will often have a pose close to its hovering stance in order to stay stable.

If the MAV rotates such that the vector $\vec{u}$ becomes noncollinear with the gravity, the cost function does not exhibit this symmetry anymore.
In this case the gyroscope bias is well estimated.

A simple solution to avoid having that symmetry in our system would be to constrain the motion of our MAV while it is operating.
Another way to artificially get rid of this symmetry is to tweak the cost function.
Specifically, we can add a regularization term that penalizes high estimations of the gyroscope bias:

\begin{equation}
cost(B) = ||\Xi X - S||^2 + \lambda ||B||
\end{equation}

The coefficient $\lambda$ is the weight given to how much we want the bias to be small.
For small values of $\lambda$, our cost function is similar to the previous one and the bias can grow arbitrarily high.
For high values of $\lambda$, the estimations provided by the optimized closed-form solution are similar to the ones provided by the standard closed-form solution.
Indeed, high values of $\lambda$ force the estimation of the bias to $0_3$.

Note that, instead of forcing the gyroscope bias to be close to $0_3$, we can easily force it to be close to any value.
Therefore, if we have the knowledge of an approximately known gyroscope bias, we can use it to provide a better estimation of the gyroscope bias.

\[
cost(B) = ||\Xi X - S||^2 + \lambda ||B - B^{approx}||
\]

With $B^{approx}$ the known approximate gyroscope bias.
This methods allow us to reuse previously computed gyroscope bias since it is known to slowly vary over time.

Selecting a reliable and safe value for the regularization parameter $\lambda$ is complicated.
In this paper, we picked a value of $\lambda$ by experimentation.

\section{VALIDATION}

We validate our method against a different dataset than the one we used in the previous sections to draw our conclusions.
Specifically, it contains IMU and camera measurements along with ground truth.
Therefore, we are now only relying on fully real terrain data.
However, since we are no longer simulating the point features, we do not have the ground truth for the distance to the point features anymore.
We can therefore only compare the performance of the evaluation of the speed and the gravity.

The drone is flying indoor at low altitude.
The feature extraction and matching is done with FAST corner algorithm as in \cite{Forster2014}.

We compare the performance on the estimations of the gravity and the initial velocity obtained with three different methods:
\begin{itemize}
\item The original closed-form solution (Equation \ref{eq:mat1});
\item Our modified closed-form solution (Equation \ref{eq:cost});
\item Fast Semi-Direct Monocular Visual Odometry (SVO) described in \cite{Forster2014}.
\end{itemize}
The reason we included the SVO in the validation is for having a reference to state of the art pose estimation method.
However, this method requires to be initialized with the knowledge of the absolute scale, whereas our method works without initialization.

We set the integration duration for the closed-form solution to 2.8 seconds.
The camera provides 60fps, but we discard most of the frames and consider only one frame every 0.1 seconds.
Indeed, considering frames that are too close to each other does not add significant information to our system.

\begin{figure}
   \centering
   \resizebox{0.7\columnwidth}{!}{\input{graph/results2.8s-CF3-CF3opt-KF.tex}}
   \caption{Estimation error of the optimized closed-form solution against the original closed-form solution \cite{Martinelli2014} and SVO \cite{Forster2014}. The duration of integration is set to 2.8 seconds, and 10 point features are observed throughout the whole operation.}
\end{figure}


%In a real applications, a proper online computation of the gyroscope bias can be reused.
%% Since the gyroscope bias varies slowly over time, we can reuse its knowledge.
%% In other words, we can use the optimized closed-form solution to compute the bias at the beginning of the sequence.
%% Thereafter, we can quickly provide reliable state estimations with the standard closed-form solution by compensating with the computed bias.

%% For long term navigation (longer than a minute), it would be advised to recompute the gyroscope bias every so often.
%% This re-computation can use the knowledge of previously computed bias. % as stated in Section \ref{sec:reg}.

%% The reason why we do not run the optimization for every state estimation is to speed up the computation.
%% Despite the optimization is relatively fast since our cost function is defined only with respect to the gyroscope bias, it still has to run the closed-form solution several times for its gradient descent to converge.
%% We found by experimentation that it usually requires around 6 iterations to converge.


\section{CONCLUSION}

For a MAV, limiting the number of on-board sensor is important to save power consumption and processing power.
A popular choice of sensors is a camera coupled with an IMU for their complementary.
However, the methods for fusing visual and inertial measurements so far introduced are filter based, hence require an initialization.
Providing a reliable state initialization is critical for these algorithms to work correctly.

In this paper, we have studied the recent closed-form solution proposed by \cite{Martinelli2014} that performs visual-inertial sensor fusion without requiring an initialization.
We implemented this method in order to test it with real terrain data.
This allowed us to identify its performance bottlenecks and bring modifications to overcome them.

We investigated the impact of biased inertial measurements.
Despite the case of biased accelerometer was originally studied in \cite{Martinelli2014} we show that its low impact on the system makes it hard to estimate.

One major performance bottleneck of this method was the impact of biased gyroscope measurements.
In other words, the performance becomes very poor in presence of a bias on the gyroscope and, in practice, the overall method could only be successfully used with a very precise - and expensive - gyroscope.
We then introduced a simple method that automatically estimates this bias.

%% However, this method requires a significant amount of data to provide correct estimates of the gyroscope bias (either many features or long integration duration).
%% We were able to get rid of this limitation with a simple statistical trick by adding a regularization term to our cost function.
%% %tweak in the cost function we are minimizing.

%% By adding this new method for the bias estimation to the original method we obtain results which are equivalent to the ones in absence of bias.
%% Compared to the original method, the new method is now robust to the gyroscope bias, and also provides the gyroscope bias.

We validated this method by comparing its performance against the original method and the SVO described in \cite{Forster2014} which is the state of the art approach for pose estimation on MAV.

For future work, we see this optimized closed-form solution being implemented on a MAV to provide accurate state initialization.
This would allow aggressive take-off maneuvers, such as hand throwing the MAV in the air \cite{Faessler2015}.
With our technique however, we could get rid of the altimeter sensor.
The drone could therefore perform any motion right after the throw instead of having a required hovering stage to compute the absolute scale.

\bibliographystyle{plain}
\bibliography{library}

\end{document}
