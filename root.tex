%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

%% \IEEEoverridecommandlockouts                              % This command is only needed if
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage [vscale=0.76,includehead]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{fullpage}
\usepackage{mathptmx} % font = times
\usepackage{helvet} % font sf = helvetica
\usepackage[latin1]{inputenc}
\usepackage{relsize}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{pgfplots}
\newcommand{\rot}[2]{\ensuremath{C_{\,#1}^{\,#2}}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\definecolor{amethyst}{rgb}{0.5, 0.3, 0.7}

% and optionally (as of Pgfplots 1.3):
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}

\title{\LARGE \bf
Absolute scale velocity determination combining visual and inertial measurements for micro aerial vehicles
}


\author{Jacques Kaiser and Agostino Martinelli% <-this % stops a space
%% \thanks{*This work was not supported by any organization}% <-this % stops a space
%% \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%%         University of Twente, 7500 AE Enschede, The Netherlands
%%         {\tt\small albert.author@papercept.net}}%
%% \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
%%         Dayton, OH 45435, USA
%%         {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Hi

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{INTRODUCTION}




Autonomous mobile robots navigating in unknow environments have an intrinsic need to perform localization and mapping using only on-board sensors.
Concerning Micro Aerial Vehicles (MAV), a critical issue is to limit the number of on-board sensors to reduce weight and power consumption.
Therefore, a common setup is to combine a monocular camera with an inertial measurements unit (IMU).
On top of being cheap, these sensors have very interesting complementarities.
Additionaly, they can operate in indoor environments where Global Positioning System (GPS) signals are shadowed.
An open question is how to optimally fuse the information provided by these sensors.

Currently, most sensor fusion algorithms are either filter based or iterative. That is, given a current state and measurements, they return an updated state.
While working well in practice, these algorithms need to be provided by an external initial state.

The initialization of a filter based method is critical.
Due to nonlinearities of the system, a poor initialization can result into converging towards local minima and  providing faulty states with high confidence.
Indeed, another shortcoming of filters is that they can silently fail.

In this work we demonstrate the efficiency of a recent closed-form solution introduced in \cite{Martinelli2012}\cite{Martinelli2014} that fuses visual and inertial data to obtain the structure of the environment at the global scale along with the attitude and the speed of the robot.

By nature, a closed-form solution is deterministic and thus does not require any initialization.
It is assumed that the camera is calibrated and the transformation between the IMU and the camera is known.
This is a fair assumption for industrial drones to come pre-calibrated.

In this work, we have studied the recent closed-form solution proposed by \cite{Martinelli2014} that performs visual-inertial sensor fusion without requiring an initialization.
We implemented this method in order to test it with real terrain data.
This allowed us to identify its bottlenecks and bring modifications to overcome them.

Specifically, we started by reformulating the equations for greater numerical stability.
This lead to a major leap in the quality of the estimations.

We then investigated the impact of biased inertial measurements.
Despite the case of biased accelerometer was originally studied in \cite{Martinelli2014} we show that its low impact on the system makes it hard to estimate.

One major bottleneck of this method was the impact of biased gyroscope measurements.
In other words, the performance becomes very poor in presence of a bias on the gyroscope and, in practice, the overall method could only be successfully used with a very precise - and expensive - gyroscope.
We then introduced a simple method that automatically estimates this bias.

However, this method requires a significant amount of data to provide correct estimates of the gyroscope bias (either many features or long time of integration).
We were able to get rid of this limitation with a simple statistical trick by adding a regularization term to our cost function.
%tweak in the cost function we are minimizing.

By adding this new method for the bias estimation to the original method we obtain results which are equivalent to the ones in absence of bias.
Compared to the original method, the new method is now robust to the gyroscope bias, and also provides the gyroscope bias.
%% Specifically, this method can accurately estimate the initial speed, the gravity, the distance to the features and the gyroscope bias for very short time of integration (around 2 seconds) and low amount of observed point features (around 5).


%% \begin{figure}[h!]
%%         \centering
%%    \resizebox{0.4\textwidth}{!}{\input{./graph}}
%%         \caption{Estimation error of the original formulation of the Closed-Form solution against the improved Closed-Form solution observing 7 features over 3 seconds.}
%% \end{figure}

\section{RELATED WORK}
\section{THE CLOSED-FORM SOLUTION}

In this paper, we do not provide a new derivation of the closed-form solution.
Instead, we consider the latest derivation proposed in \cite{Martinelli2014}.
Specifically, the author expresses the state of the MAV with respect to the visual and inertial measurements in Equation \ref{eq:final1}:


\begin{equation} \tag{6} \label{eq:final1}
S_j = \lambda_1^i\mu_1^i - V t_j - G \frac{t_j^2}{2} - \lambda^i_j \mu^i_j
\end{equation}
With:
\begin{itemize}
\item $\mu_j^i$ the normalized bearing of point feature $i$ at time $t_j$ in the initial local frame;
\item $\lambda_j^i$ the distance to the point feature $i$ at time $t_j$;
\item $V$ the initial velocity in the initial local frame;
\item $G$ the initial gravity in the initial local frame;
\item $S_j$ the integration up to time $t_j$ of the rotated linear acceleration data.
\end{itemize}



The unknowns of Equation \ref{eq:final1} are the scalars $\lambda_j^i$ and the vectors $V$ and $G$.
Note that the knowledge of $G$ is equivalent to the knowledge of the roll and pitch angles.
The vectors $\mu_j^i$ are fully determined by camera observations and gyroscope measurements,
and the vectors $S_j$ are determined by accelerometer and gyroscope measurements.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.90\columnwidth]{images/closedFormExplained}
  \caption{Visual representation of Equation \ref{eq:final1}.
  The unknowns of the equation are colored in \textcolor{amethyst}{purple}.}
\end{figure}

Equation \ref{eq:final1} holds for each three dimensions of all point features $i=1,...,N$ and each observation starting from the second one $j=2,...n_i$.
We therefore have a linear system consisting of $3(n_i-1)N$ equations in $6 + Nn_i$ unknowns.
Indeed, note that when the first observation occurs, at $t_j = 0$,
Equation \ref{eq:final1} is always satisfied thus does not provide information.
We can write our system using matrix formulation.
Solving the system is equivalent to inverting a matrix of $3(n_i-1)N$ rows and $6+Nn_i$ columns.

In \cite{Martinelli2014}, the author proceeded to one more step before expressing the underlying linear system.
For an observation at time $t_j$, the equation of the first point feature $i=1$ happening at time $t_j$ is subtracted to all other point features $1<i<=N$ at time $t_j$ (Equation 7).
This additional step has the effect to corrupt all measurements with the first measurement,
hence worsening the performance of the closed-form solution.
In this paper, we do not take to this additional step.

The linear system in Equation \ref{eq:final1} can be written in the following compact format:

\begin{equation}
\label{eq:mat1} \tag{9}
\Xi X = S
\end{equation}

The matrix $\Xi$ and the vector $S$ are fully determined by the measurements, while $X$ is the unknown vector.
We have:

\[
S \equiv [S_2^T, ...,S_2^T, S_3^T,...,S_3^T,...,S_{n_i}^T,...,S_{n_i}^T]^T
\]
\[
X \equiv [ G^T, V^T, \lambda_1^1, ..., \lambda_1^N, ..., \lambda_{n_i}^1, ..., \lambda_{n_i}^N]^T \\
\]

\begin{multline}
  \Xi \equiv \\
  \left[
    {\scriptscriptstyle
    \begin{array}{l|l|l|l|l|l|l|l|l|l|l}
      T_2 & S_2 & \mu_1^1 & 0_3 & 0_3 & -\mu_2^1 & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 \\
      T_2 & S_2 & 0_3 & \mu_1^2 & 0_3 & 0_3 & -\mu_2^2 & 0_3 & 0_3 & 0_3 & 0_3 \\
      ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
      T_2 & S_2 & 0_3 & 0_3 & \mu_1^N & 0_3 & 0_3 & -\mu_2^N & 0_3 & 0_3 & 0_3 \\
      ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
      ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
      T_{n_i} & S_{n_i} & \mu_1^1 & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 & -\mu_{n_i}^1 & 0_3 & 0_3 \\
      T_{n_i} & S_{n_i} & 0_3 & \mu_1^2 & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 & -\mu_{n_i}^2 & 0_3 \\
      ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
      T_{n_i} & S_{n_i} & 0_3 & 0_3 & \mu_1^N & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 & -\mu_{n_i}^N
    \end{array}
    }
    \right]
\end{multline}

Where $T_j \equiv - \frac{t^2_j}{2} I_3$, $S_j \equiv -t_j I_3$ and $I_3$ is the identity 3 x 3 matrix; $0_{33}$ is the 3 x 3 zero matrix.
Note that the matrix $\Xi$ and the vector $S$ are slightly different from the one proposed in \cite{Martinelli2014}.
This is due to the additional step we did not take for numerical stability reasons.

The sensor information is completely contained in the above linear system. Additionally, in \cite{Martinelli2014}, the author added a quadratic equation assuming the gravitational acceleration is a priori known.
Let us denote the gravitational magnitude by $g$.
We have the extra constraint $|G| = g$. We can express this constraint in matrix formulation:

\begin{equation}
\label{eq:mat2} \tag{10}
| \Pi X | ^2 = g^2
\end{equation}

With $\Pi \equiv [I_3, 0_3, ..., 0_3]$.

We can therefore recover the initial velocity, the roll and pitch angles and the distances to the point features
by finding the vector $X$ which satisfies \ref{eq:mat1} and \ref{eq:mat2}.

In the next sections, we will evaluate the performance of this method on real terrain data.
This will allow us to identify its weaknesses and bring modifications to overcome them.

%% The system therefore becomes:

%% \begin{equation} \label{eq:final2}
%% \left[
%% \begin{array}{lcl}
%% S_j &=& \lambda_1^i\mu_1^i - V t_j - G \frac{t_j^2}{2} - \lambda^i_j \mu^i_j\\
%% 0_3 &=& \lambda_1^1\mu_1^1 - \lambda_j^1\mu_j^1 - \lambda_1^i\mu_1^i + \lambda^i_j \mu^i_j
%% \end{array}
%% \right.
%% \end{equation}



\section{PERFORMANCE BOTTLENECKS}

\subsection{Test setup}

The MAV performs a motion while being tracked with an optical Vicon system.
We can therefore compare our estimations with the ground truth.
We define the relative error as the euclidian distance between the estimation and the ground truth,
normalized by the groud truth.
We measure our error on the absolute scale by computing the mean error over all estimated distances to point features $\lambda_j^i$.

To identify the performance bottlenecks, we used IMU data obtained from terrain acquisitions
while we simulated the point feature observations.
This separation allowed us to know the ground truth for the distance to the point features
and also better understand the weaknesses of our method.

We represent this setup in Fig. \ref{fig:testsetup}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\columnwidth]{images/setupTestDroneError.png}
  \caption{Test setup for identifying the performance bottlenecks.
    The drone is equipped with an IMU, and the visual measurements are simulated.
    It performs a motion while being tracked by a Vicon system.\label{fig:testsetup}}
\end{figure}

In general, we use one camera observation every 0.3 seconds, even if the camera provides significantly more frames.
Indeed, we can discard most of the camera observations.
If two observations are too close to each other, then the additional equations do not bring much information to our system.
Reducing the number of considered frames reduces the size of the matrices, thus speeds up the computations.

As an example, over a time interval of 3 seconds, we obtain $11$ distinct frames.
When observing 7 features, it yields a system of $3\times 10\times 7 = 210$ equations and $6+7\times 10=76$ unknowns.

The method we use to solve the overconstrained linear system $\Xi X = S$ is a singular value decomposition (SVD) since it yields numerically robust solutions.

In this section, we will start by presenting the results obtained with the original closed-form solution on terrain IMU data.
Our goal is to identify its performance bottlenecks and introduce modifications to overcome them.

%% The case of a biased gyroscope is addressed in chapter \ref{ch:gyro}.
%% Indeed, the main contribution of this report is the simple method introduced in Section \ref{sec:computeBias} to compute the gyroscope bias with the closed-form solution, which happened to be a major performance bottleneck.

\subsection{Original closed-form solution performance}

The original closed-form solution described in Equation \ref{eq:mat1} will be used as a basis for our work.
Moreover, we can also use the knowledge of the gravity magnitude in order to refine our results with Equation \ref{eq:mat2}.
In this case, we are minimizing a linear objective function with a quadratic constraint.
in Fig. \ref{fig:original}, we represent the quality of the evaluations with and without this additional constraint.


\begin{figure}[h!]
  \centering
    \resizebox{0.6\columnwidth}{!}{\input{graph/droneMotion14.6for5s}}
    \caption{Motion performed by the drone in 5 seconds.}
\end{figure}

\begin{figure}[h!]
  \centering
    \resizebox{0.7\columnwidth}{!}{\input{graph/originalCF}}
    \caption{Original closed-form solution estimations with and without gravity knowledge refinement. We are observing 5 features  over a variable duration of integration.\label{fig:original}}
\end{figure}


Note how the evaluations get better as we increase the integration duration.
Indeed, our equations come from an extended triangulation \cite{Martinelli2012}.
Therefore, it requires a significant difference in the measurements over time to robustly estimate the state.
Also note that the gravity is robustly estimated (around 5\% error), whereas the speed and the distance to the features is more erroneous (above 10\% error).

Since the gravity is well estimated with the original closed-form, it comes without surprise that constraining its magnitude does not improve the performance much.
The distance to the features are slightly improved (around 1\% error decrease) at the expense of worsening estimation of the speed (around 20\% error increase).

%% In the following sections, we will introduce modifications to improve over these results.
In the following sections, we will study the impact of biased inertial measurements on the performance of the closed-form solution.

\subsection{Impact of accelerometer bias on the performance}
In order to visualize the impact of the accelerometer bias on the performance,
we corrupt the accelerometer measurements provided by our terrain IMU by adding an artificial bias (Fig. \ref{fig:biasAccCF}).

Despite a high accelerometer bias the closed-form solution still provides robust results.

\begin{figure}[h!]
        \centering

        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasGravity}}
                \caption{Gravity estimation error}

        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasSpeed}}
                \caption{Velocity estimation error}

        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasLambda}}
                \caption{Lambda estimation error}

        \end{subfigure}
        \caption{Impact of the accelerometer bias on the performance of the closed-form solution. We are observing 7 features  over a variable duration of integration.\label{fig:biasAccCF}}
\end{figure}

As seen on the Fig. \ref{fig:biasAccCF}, neither the estimation of the gravity, the velocity or the lambdas are impacted by the accelerometer bias.

In \cite{Martinelli2014}, the author provides an alternative formulation of the closed-form solution including the accelerometer bias as an observable unknown of the system.
However, the estimation of the accelerometer bias with this method is not robust since our system is only slightly affected by it.

This is a counterintuitive results. Since our equations contain an integration of the acceleration, we also perform an integration of the accelerometer bias.
We would have expected the accelerometer bias to have a greater impact on the solutions yielded by the system.



\subsection{Impact of gyroscope bias on the performance}
Again, in order to visualize the impact of the gyroscope bias on the performance,
we corrupt the gyroscope measurements provided by our terrain IMU by adding an artificial bias (Fig. \ref{fig:biasGyroCF}).

Our experiments have shown that the presence of gyroscope bias significantly damages the results of the closed-form solution.

\begin{figure}[h!]
        \centering

        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasGravity}}
                \caption{Gravity estimation error}

        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasSpeed}}
                \caption{Velocity estimation error}

        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasLambda}}
                \caption{Lambda estimation error}

        \end{subfigure}
        \caption{Impact of the gyroscope bias on the performance of the closed-form solution. We are observing 7 features over a variable duration of integration. \label{fig:biasGyroCF}}
\end{figure}

As seen in Fig. \ref{fig:biasGyroCF}, the performance becomes very poor in presence of a bias on the gyroscope and, in practice, the overall method could only be successfully used with a very precise---and expensive---gyroscope.



\section{ESTIMATING THE GYROSCOPE BIAS}

Previous work has shown that the gyroscope bias is an observable mode when using an IMU and a camera, which means that it can be estimated \cite{Martinelli2012}.

Optimally, we would add the gyroscope bias in our unknown vector $X$ and determine $X$ by simply inverting the system $\Xi X = S$ as in the standard closed-form solution.
However, we can not express the gyroscope bias linearly with this system.

In this section, we propose a different approach to estimate the gyroscope bias using the closed-form solution.

\subsection{Nonlinear minimization of the residual}

Since our system of equations (\ref{eq:final1}) is overconstrained,
inverting it is equivalent to finding the vector $X$ that minimizes the residual $||\Xi X - S||^2$.

Since we can not express the gyroscope bias linearly,
we define the following cost function:

\begin{equation}
  \label{eq:cost}
  cost(B) = ||\Xi X - S||^2
\end{equation}

With:
\begin{itemize}
\item $B$ the gyroscope bias;
\item $\Xi$ and $S$ computed with respect to $B$.
\end{itemize}

By minimizing this cost function, we recover the gyroscope bias $B$ and the unknown vector $X$ which compensated for the gyroscope bias $B$.
We can initialize the optimization process with $B = 0_3$ since the bias is usually a rather small quantity.

\begin{figure}[h!]
  \centering
  \resizebox{0.7\columnwidth}{!}{\input{graph/biasEstimation30Ft}}
  \caption{Gyroscope bias estimation from nonlinear minimization of the residual. We are observing 30 features  over a variable duration of integration. The true gyroscope bias is $[0.0276,   -0.0024,    0.0417]$. \label{fig:gBiasEstimate}}
\end{figure}

\begin{figure}[h!]
   \centering
   \resizebox{0.7\columnwidth}{!}{\input{graph/CF3vsOpt30Ft}}
                \caption{Estimation error of the closed-form solution against the optimized closed-form solution.  We are observing 30 features  over a variable duration of integration. The true gyroscope bias is $[0.0276,   -0.0024,    0.0417]$. \label{fig:optEstimate}}
\end{figure}

\begin{figure}[h!]
        \centering

        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/opt30ftgravity}}
                \caption{Gravity estimation relative error}

        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/opt30ftspeed}}
                \caption{Velocity estimation relative error}

        \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/opt30ftLambda}}
                \caption{Lambda estimation relative error}

        \end{subfigure}
        \caption{Impact of the gyroscope bias on the performance of the optimized closed-form solution. We are observing 7 features  over a variable duration of integration. \label{fig:biasGyroOpt}}
\end{figure}

%% This technique is similar to the one used in \cite{Abbeel2005}, where the author minimizes the residual of a Kalman filter in order to recover the noise matrix for the sensors.

The optimized closed-form solution provides better results than the standard closed-form solution. Fig. \ref{fig:optEstimate} depicts an improvement in precision of around $5\%$ for the distance to the features, and around $13\%$ for the speed after 4 seconds of integration.

Moreover, the optimized closed-form solution requires a shorter integration duration to provide good quality results.
Specifically, after 2 seconds of integration (around 7 camera observations) the provided estimations are already robust.
The non-optimized closed-form solution requires 3 seconds of integration before converging to acceptable estimations.


Lastly, this method is robust even for high values of the gyroscope bias.
Fig. \ref{fig:biasGyroOpt} represents the quality of the estimations with the same artificial gyroscope bias from Fig. \ref{fig:biasGyroCF}.



As seen in Fig. \ref{fig:biasGyroOpt}, after a certain integration duration, the estimations agree no matter how high the bias is.
In other words, given that the integration duration is long enough, this method is unaffected by the gyroscope bias.
%% This method can therefore fully compensate for high values of the gyroscope bias.
%% Moreover, the values on which these evaluations agree are equals to the ones for the curve that has a gyroscope bias set to $0_3$.


By adding this new method for the bias estimation to the original method we obtain results which are equivalent to the ones in absence of bias.
Compared to the original method, the new method is now robust to the gyroscope bias, and also provides the gyroscope bias.

\section{VALIDATION}

We validate our method against a different dataset than the one we used in the previous sections to draw our conclusions.
Specifically, it contains IMU and camera measurements along with ground truth.
Therefore, we are now only relying on fully real terrain data,
but also we do not have the ground truth for the distance to the point features anymore.
The drone is flying indoor at low altitude.
The feature extraction and matching is done with FAST corner algrotihm as in \cite{Forster2014}.

We compare the performance on the estimations of the gravity and the initial velocity obtained with three different methods:
\begin{itemize}
\item The original closed-form solution (Equation \ref{eq:mat1});
\item Our modified closed-form solution (Equation \ref{eq:cost});
\item Fast Semi-Direct Monocular Visual Odometry (SVO) described in \cite{Forster2014}.
\end{itemize}
The reason we included the SVO in the validation is for having a reference to state of the art visual inertial fusion method.
However, this method requires to be initialized with the knowledge of the absolute scale, whereas our method works without initialization.

We set the integration duration for the closed-form solution to 2 seconds.
The camera provides 60fps, but we discard most of the observations and consider only one observation every 0.3 seconds.
Indeed, adding observations that are too close to each other does not add significant information to our system.

\begin{figure}[h!]
   \centering
   \resizebox{0.7\columnwidth}{!}{\input{graph/results2.8s-CF3-CF3opt-KF.tex}}
                %% \caption{Estimation error of the closed-form solution against the optimized closed-form solution.  We are observing 30 features  over a variable duration of integration. The true gyroscope bias is $[0.0276,   -0.0024,    0.0417]$. \label{fig:optEstimate}}
\end{figure}


%In a real applications, a proper online computation of the gyroscope bias can be reused.
%% Since the gyroscope bias varies slowly over time, we can reuse its knowledge.
%% In other words, we can use the optimized closed-form solution to compute the bias at the beginning of the sequence.
%% Thereafter, we can quickly provide reliable state estimations with the standard closed-form solution by compensating with the computed bias.

%% For long term navigation (longer than a minute), it would be advised to recompute the gyroscope bias every so often.
%% This re-computation can use the knowledge of previously computed bias. % as stated in Section \ref{sec:reg}.

%% The reason why we do not run the optimization for every state estimation is to speed up the computation.
%% Despite the optimization is relatively fast since our cost function is defined only with respect to the gyroscope bias, it still has to run the closed-form solution several times for its gradient descent to converge.
%% We found by experimentation that it usually requires around 6 iterations to converge.


\section{CONCLUSION}


\bibliographystyle{plain}
\bibliography{library}

\end{document}
