% !TeX TXS-program:compile = txs:///pdflatex/[--shell-escape]
%pdflatex -shell-escape root.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts % This command is only needed if you want to use the \thanks command
%% \overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage [vscale=0.76,includehead]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{fullpage}
\usepackage{mathptmx} % font = times
\usepackage{helvet} % font sf = helvetica
\usepackage[latin1]{inputenc}
\usepackage{relsize}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage[backend=bibtex,bibstyle=ieee,citestyle=numeric-comp]{biblatex}
\bibliography{library}

%% \setlength{\belowdisplayskip}{1.5pt} \setlength{\belowdisplayshortskip}{1.5pt}
\setlength{\abovedisplayskip}{1.7pt} \setlength{\abovedisplayshortskip}{1.7pt}
\setlength{\textfloatsep}{8pt plus 1.0pt minus 2.0pt}
\setlength{\floatsep}{8pt plus 1.0pt minus 2.0pt}
\setlength{\intextsep}{8pt plus 1.0pt minus 2.0pt}

\usepgfplotslibrary{external}
\tikzexternalize[prefix=TikzPictures/]

\newenvironment{customlegend}[1][]{%
    \begingroup
    % inits/clears the lists (which might be populated from previous
    % axes):
    \csname pgfplots@init@cleared@structures\endcsname
    \pgfplotsset{#1}%
}{%
    % draws the legend:
    \csname pgfplots@createlegend\endcsname
    \endgroup
}%

% makes \addlegendimage available (typically only available within an
% axis environment):
\def\addlegendimage{\csname pgfplots@addlegendimage\endcsname}
\def\addlegendentry{\csname pgfplots@addlegendentry\endcsname}

\renewcommand*{\bibfont}{\fontsize{7.999}{9}\selectfont}

\newcommand{\rot}[2]{\ensuremath{C_{\,#1}^{\,#2}}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\definecolor{amethyst}{rgb}{0.5, 0.3, 0.7}

% and optionally (as of Pgfplots 1.3):
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}

\title{Simultaneous State Initialization and Gyroscope Bias Calibration in Visual Inertial aided Navigation} %Use for final RAL version




%% \author{Jacques Kaiser$^{1}$, Agostino Martinelli$^{1}$, Flavio Fontana$^{2}$ and Davide Scaramuzza$^{2}$% <-this % stops a space
%%  \thanks{*This work was supported by the French National Research
%% Agency ANR through the project VIMAD}% <-this % stops a space
%%  \thanks{$^{1}$ INRIA Rhone Alpes, Grenoble, France. Email: jacko.kaiser@gmail.com, agostino.martinelli@ieee.org}%
%%  \thanks{$^{2}$ Robotics and Perception Group, University of Zurich, Switzerland. Email: ffontana,sdavideg@ifi.uzh.ch}%
%% }

% Paper headers
\markboth{IEEE Robotics and Automation Letters. Preprint Version. Accepted January, 2016}{Kaiser \MakeLowercase{\textit{et al.}}: State Initialization and Gyro Bias Calibration}


% Make room for more info lines in the \author command
\author{Jacques Kaiser$^{1}$, Agostino Martinelli$^{1}$, Flavio Fontana$^{2}$ and Davide Scaramuzza$^{2}$%
\thanks{Manuscript received: August, 31, 2015; Revised September, 29, 2015; Accepted January, 8, 2016.}%Use only for final RAL version
\thanks{This paper was recommended for publication by Editor Antonio Bicchi upon evaluation of the Associate Editor and Reviewers' comments. *This work was supported by the French National Research
Agency ANR through the project VIMAD.}%Use only for final RAL version
\thanks{$^{1}$ INRIA Rhone Alpes, Grenoble, France. {\tt\small jacko.kaiser@gmail.com, agostino.martinelli@ieee.org}}%
\thanks{$^{2}$ Robotics and Perception Group, University of Zurich, Switzerland. {\tt\small ffontana,sdavideg@ifi.uzh.ch}}%
\thanks{Digital Object Identifier (DOI): see top of this page.}
}
% Use only for final RAL version


\begin{document}



\maketitle
%% \thispagestyle{empty}
%% \pagestyle{empty}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
State of the art approaches for visual-inertial sensor fusion use filter-based or optimization-based algorithms. Due to the nonlinearity of the system, a poor initialization can have a dramatic impact on the performance of these estimation methods.
Recently, a closed-form solution providing such an initialization was derived in \cite{Martinelli2014}.
That solution determines the velocity (angular and linear) of a monocular camera in metric units by only using inertial measurements and image features acquired in a short time interval.
In this paper, we study the impact of noisy sensors on the performance of this closed-form solution. We show that the gyroscope bias, not accounted for in \cite{Martinelli2014}, significantly affects the performance of the method.
Therefore, we introduce a new method to automatically estimate this bias.
Compared to the original method, the new approach now models the gyroscope bias and is robust to it.
The performance of the proposed approach is successfully demonstrated on real data from a quadrotor MAV.
\end{abstract}

\begin{IEEEkeywords}
  Sensor Fusion, Localization, Visual-Based Navigation
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{INTRODUCTION}

% Drop letter for first word of the Introduction
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{A}{utonomous} mobile robots navigating in unknown environments have an intrinsic need to perform localization and mapping using only on-board sensors.
Concerning Micro Aerial Vehicles (MAV), a critical issue is to limit the number of on-board sensors to reduce weight and power consumption.
Therefore, a common setup is to combine a monocular camera with an inertial measurements unit (IMU).
On top of being cheap, these sensors have very interesting complementarities.
Additionally, they can operate in indoor environments, where Global Positioning System (GPS) signals are shadowed.
An open question is how to optimally fuse the information provided by these sensors.

Currently, most sensor-fusion algorithms are either filter-based or iterative. That is, given a current state and measurements, they return an updated state.
While working well in practice, these algorithms need to be provided with an initial state.
The initialization of these methods is critical.
Due to nonlinearities of the system, a poor initialization can result into converging towards local minima and  providing faulty states with high confidence.
%% Indeed, another shortcoming of filters is that they can silently fail.

In this paper, we demonstrate the efficiency of a recent closed-form solution introduced in \cite{Martinelli2012, Martinelli2014}, which fuses visual and inertial data to obtain the structure of the environment at the global scale along with the attitude and the speed of the robot.
By nature, a closed-form solution is deterministic and, thus, does not require any initialization.

%% It is assumed that the camera is calibrated and the transformation between the IMU and the camera is known.
%% This is a fair assumption for industrial drones to come pre-calibrated.

The method introduced in \cite{Martinelli2012, Martinelli2014} was only described in theory and demonstrated with simulations on generic Gaussian motions, not plausible for an MAV.
In this paper, we perform simulations with plausible MAV motions and synthetic noisy sensor data.
Our simulations are therefore closer to the real dynamics of an MAV.
This allows us to identify limitations of the method and bring modifications to overcome them.
Specifically, we investigate the impact of biased inertial measurements.
Although the case of biased accelerometer was originally studied in \cite{Martinelli2014}, here we show that a large bias on the accelerometer does not significantly worsen the performance.
One major limitation of \cite{Martinelli2014} is the impact of biased gyroscope measurements.
In other words, the performance becomes very poor in presence of a bias on the gyroscope and, in practice, the overall method can only be successfully used with a very precise - and expensive - gyroscope.
Here, we introduce a simple method that automatically estimates this bias. By adding this new method for the bias estimation to the original method \cite{Martinelli2014}, we obtain results that are equivalent to the ones in absence of bias.
This method is suitable for dynamic take off and on-the-fly re-initialisation since it does not require a calibration step with the MAV sitting stationary.
Compared to \cite{Martinelli2014}, the new method is now robust to the gyroscope bias and automatically calibrates the gyroscope.



%% Specifically, this method can accurately estimate the initial speed, the gravity, the distance to the features and the gyroscope bias for very short time of integration (around 2 seconds) and low amount of observed point features (around 5).


%% \begin{figure}[h!]
%%         \centering
%%    \resizebox{0.4\textwidth}{!}{\input{./graph}}
%%         \caption{Estimation error of the original formulation of the Closed-Form solution against the improved Closed-Form solution observing 7 features over 3 seconds.}
%% \end{figure}

\section{RELATED WORK}

The problem of fusing visual and inertial data has been extensively investigated in the past.
However, most of the proposed methods require a state initialization.
Because of the system nonlinearities, lack of precise initialization can irreparably damage the entire estimation process.
In literature, this initialization is often guessed or assumed to be known \cite{Armesto2007, Li2013, Huang2009, Bibuli2007, Forster2014}. Recently, this sensor fusion problem has been successfully addressed by enforcing observability constraints \cite{Hesch2014, Huang2015} and by using optimization-based approaches \cite{Leute2014, Forster2015, mourikis2008dual, lupton2012visual, huang2011observability, mourikis2007multi, Indelman2013}. These optimization methods outperform filter-based algorithms in terms of accuracy due to their capability of relinearizing past states. On the other hand, the optimization process can be affected by the presence of local minima.
We are therefore interested in a deterministic solution that analytically expresses the state in terms of the measurements provided by the sensors during a short time-interval.

In computer vision, several deterministic solutions have been introduced.
These techniques, known as {\it Structure from Motion}, can recover the relative rotation and translation up to an unknown scale factor between two camera poses \cite{Hartley2004}.
Such methods are currently used in state-of-the-art visual navigation methods for MAVs to initialize maps \cite{Weiss2012, Forster2014, FaesslerICRA15}.
However, the knowledge of the absolute scale, and, at least, of the absolute roll and pitch angles, is essential for many applications ranging from autonomous navigation in GPS-denied environments to 3D reconstruction and augmented reality.
For these applications, it is crucial to take the inertial measurements into consideration to compute these values deterministically.

%% Some visual inertial sensor fusion that works without initialization have been introduced.
%% In \cite{Lupton2012} the authors pre-integrate their IMU data in an arbitrary frame moving at the same velocity than the previous body frame of the MAV.
%% Since the reference frame is moving at the same instantaneous velocity as the vehicle was at the previous pose, the initial velocity with respect to this frame is zero, making integration without initial conditions possible.
%% The camera measurements are then used to match the initial reference frame with the defined reference frame.
%% Their method can recover the absolute scale, roll and pitch angles, in a linear manner.
%% However, their method is purely numerical and we can not analytically derive its properties.

%, therefore no assumptions can be made about its properties.

A procedure to quickly re-initialize an MAV after a failure was presented in \cite{Faessler2015}.
However, this method requires an altimeter to initialize the scale.

Recently, a closed-form solution has been introduced in \cite{Martinelli2012}.
From integrating inertial and visual measurements over a short time-interval, this solution provides the absolute scale, roll and pitch angles, initial velocity, and distance to 3D features.
Specifically, all the physical quantities are obtained by simply inverting a linear system.
The solution of the linear system can be refined with a quadratic equation assuming the knowledge of the gravity magnitude.
This closed-form was improved in \cite{DongSi2012} to work with unknown camera-IMU calibration;
however, since in this case the problem cannot be solved by simply inverting a linear system, a method to determine the six parameters that characterize the camera-IMU transformation was proposed.
As a result, this method is independent of external camera-IMU calibration, hence, suitable for power-on-and-go systems.

A more intuitive expression of this closed-form solution was derived in \cite{Martinelli2014}.
While being mathematically sound, this closed-form solution is not robust to noisy sensor data.
For this reason, to the best of our knowledge, it has never been used in an actual application.
In this paper, we perform an analysis to find out its limitations. We start by reminding the reader the basic equations that characterize this solution (section \ref{SectionCFS}).
In section \ref{SectionBottlenecks}, we show that this solution is resilient to the accelerometer bias but strongly affected by the gyroscope bias.
We then introduce a simple method that automatically estimates the gyroscope bias (section \ref{SectionCalibration}).
By adding this new method for the bias estimation to the original method, we obtain results that are equivalent to the ones obtained in absence of bias.
Compared to the original method, the new method is now robust to the gyroscope bias and also calibrates the gyroscope.
In section \ref{SectionPerformance}, we validate our new method against real world data from a flying quadrotor MAV to prove its robustness against noisy sensors during actual navigation.
Finally, we provide the conclusions in section \ref{SectionConclusion}.

\section{CLOSED-FORM SOLUTION}\label{SectionCFS}

In this section, we provide the basic equations that characterize the closed-form solution proposed in  \cite{Martinelli2014}. We also provide the main features of this solution\footnote{Note that in this paper we do not provide a new derivation of this solution for which the reader is addressed to \cite{Martinelli2014}, section 3.}.

Let us refer to a short interval of time (e.g., of the order of $3$ seconds). We assume that during this interval of time the camera observes simultaneously $N$ point-features and we denote by $t_1,~t_2,\cdots,t_{n_i}$ the times of this interval at which the camera provides an image of these points. Without loss of generality, we can assume that $t_1=0$.
The following equation holds (see \cite{Martinelli2014} for its derivation):


\begin{equation} \label{eq:final1}
S_j = \lambda_1^i\mu_1^i - V t_j - G \frac{t_j^2}{2} - \lambda^i_j \mu^i_j
\end{equation}
\noindent with:
\begin{itemize}
\item $\mu_j^i$ the normalized bearing of point feature $i$ at time $t_j$ in the local frame at time $t_1$;
\item $\lambda_j^i$ the distance to the point feature $i$ at time $t_j$;
\item $V$ the velocity in the local frame at time $t_1$;
\item $G$ the gravity in the local frame  at time $t_1$;
\item $S_j$ the integration in the interval $[t_1, ~t_j]$ of the rotated linear acceleration data (i.e., the integration of the inertial measurements).
\end{itemize}


The local frame refers to a frame of reference common to the IMU and the camera.
In a real application, we would work in the IMU frame and have some additional constant terms
accounting for the camera-IMU transformation.
We do not express these constant calibration terms explicitly here for clarity reasons.

The unknowns of Equation \ref{eq:final1} are the scalars $\lambda_j^i$ and the vectors $V$ and $G$.
Note that the knowledge of $G$ is equivalent to the knowledge of the roll and pitch angles.
The vectors $\mu_j^i$ are fully determined by visual and gyroscope measurements \footnote{The gyroscope measurements in the interval $[t_1, ~t_j]$ are needed to express the bearing at time $t_j$ in the frame at time $t_1$},
and the vectors $S_j$ are determined by accelerometer and gyroscope measurements.

\begin{figure}
  \centering
  \includegraphics[width=0.7\columnwidth, trim={0 2cm 0 0}, clip]{images/closedFormExplained}
  \caption{Visual representation of Equation \ref{eq:final1}.
  The unknowns of the equation are colored in \textcolor{amethyst}{purple}.}
\end{figure}

Equation (\ref{eq:final1}) provides three scalar equations for each point feature $i=1,...,N$ and each frame starting from the second one $j=2,...n_i$.
We therefore have a linear system consisting of $3(n_i-1)N$ equations in $6 + Nn_i$ unknowns.
Indeed, note that, when the first frame is taken at $t_1 = 0$,
Equation (\ref{eq:final1}) is always satisfied; thus does not provide information.
We can write our system using matrix formulation.
Solving the system is equivalent to inverting a matrix of $3(n_i-1)N$ rows and $6+Nn_i$ columns.

In \cite{Martinelli2014}, the author proceeded to one more step before expressing the underlying linear system.
For a given frame $j$, the equation of the first point feature $i=1$ is subtracted from all other point feature equations $1<i \leq N$ (Equation (7)).
This additional step, very useful to detect system singularities, has the effect to corrupt all measurements with the first measurement,
hence worsening the performance of the closed-form solution. Therefore,
in this paper we discard this additional step.

The linear system in Equation (\ref{eq:final1}) can be written in the following compact form:

\begin{equation}
\label{eq:mat1}
\Xi X = S.
\end{equation}
\noindent Matrix $\Xi$ and vector $S$ are fully determined by the measurements, while $X$ is the unknown vector.
We have:

\begin{equation*}
  \begin{aligned}
S &\equiv [S_2^T, ...,S_2^T, S_3^T,...,S_3^T,...,S_{n_i}^T,...,S_{n_i}^T]^T \\
X &\equiv [ G^T, V^T, \lambda_1^1, ..., \lambda_1^N, ..., \lambda_{n_i}^1, ..., \lambda_{n_i}^N]^T
  \end{aligned}
\end{equation*}
\vspace{-1cm}

{
\arraycolsep=3pt % default: 5pt
\medmuskip = 1mu % default: 4mu plus 2mu minus 4mu
\begin{multline*}
  \Xi \equiv \\
      {
        \footnotesize
        \left[
          \begin{array}{l|l|l|l|l|l|l|l|l|l|l}
            T_2 & S_2 & \mu_1^1 & 0_3 & 0_3 & -\mu_2^1 & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 \\
            T_2 & S_2 & 0_3 & \mu_1^2 & 0_3 & 0_3 & -\mu_2^2 & 0_3 & 0_3 & 0_3 & 0_3 \\
            ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
            T_2 & S_2 & 0_3 & 0_3 & \mu_1^N & 0_3 & 0_3 & -\mu_2^N & 0_3 & 0_3 & 0_3 \\
            ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
            ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
            T_{n_i} & S_{n_i} & \mu_1^1 & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 & -\mu_{n_i}^1 & 0_3 & 0_3 \\
            T_{n_i} & S_{n_i} & 0_3 & \mu_1^2 & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 & -\mu_{n_i}^2 & 0_3 \\
            ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... \\
            T_{n_i} & S_{n_i} & 0_3 & 0_3 & \mu_1^N & 0_3 & 0_3 & 0_3 & 0_3 & 0_3 & -\mu_{n_i}^N
          \end{array}
          \right],
      }
\end{multline*}
}
\noindent where $T_j \equiv - \frac{t^2_j}{2} I_3$, $S_j \equiv -t_j I_3$ and $I_3$ is the identity $3\times 3$ matrix, $0_3$ is the $3\times 1$ zero matrix.
Note that matrix $\Xi$ and vector $S$ are slightly different from the ones proposed in \cite{Martinelli2014}.
This is due to the additional step that, as we explained in the previous paragraph, we discarded for numerical stability reasons (see \cite{Martinelli2014} section 3 for further details).

The sensor information is completely contained in the above linear system. Additionally, in \cite{Martinelli2014}, the author added a quadratic equation assuming the gravitational acceleration is a priori known.
Let us denote the gravitational magnitude by $g$.
We have the extra constraint $|G| = g$. We can express this constraint in matrix formulation:

\begin{equation}
\label{eq:mat2}
| \Pi X | ^2 = g^2,
\end{equation}

\noindent with $\Pi \equiv [I_3, 0_3, ..., 0_3]$. We can therefore recover the initial velocity, the roll and pitch angles, and the distances to the point features
by finding the vector $X$ that satisfies (\ref{eq:mat1}) and (\ref{eq:mat2}).

In the next sections, we will evaluate the performance of this method on simulated noisy sensor data.
This will allow us to identify its weaknesses and bring modifications to overcome them.

%% The system therefore becomes:

%% \begin{equation} \label{eq:final2}
%% \left[
%% \begin{array}{lcl}
%% S_j &=& \lambda_1^i\mu_1^i - V t_j - G \frac{t_j^2}{2} - \lambda^i_j \mu^i_j\\
%% 0_3 &=& \lambda_1^1\mu_1^1 - \lambda_j^1\mu_j^1 - \lambda_1^i\mu_1^i + \lambda^i_j \mu^i_j
%% \end{array}
%% \right.
%% \end{equation}



\section{LIMITATIONS OF \cite{Martinelli2014}}\label{SectionBottlenecks}

The goal of this section is to find out the limitations of the solution proposed in \cite{Martinelli2014} when it is adopted in a real scenario. In particular, special attention will be devoted to the case of an MAV equipped with low-cost camera and IMU sensors. For this reason, we perform simulations that significantly differ from the ones performed in \cite{Martinelli2014} (section $5.2$). Specifically, they differ because of the following two reasons:
\begin{itemize}
\item the simulated motion is the one of an MAV;
\item the values of the biases are significantly larger than the ones in \cite{Martinelli2014}.
\end{itemize}
This will allow us to evaluate the impact of the bias on the performance.

\subsection{Simulation setup}\label{SubsectionConsidered}

We simulate an MAV as a point particle executing a circular trajectory of about $1$m radius.

We measure our error on the absolute scale by computing the mean error over all estimated distances to point features $\lambda_j^i$.
We define the relative error as the euclidean distance between the estimation and the ground truth,
normalized by the ground truth.

Synthetic gyroscope and accelerometer data are affected by a statistical error of $0.5$ deg/s and $0.5$ cm/s\textsuperscript{2}, respectively and they are also corrupted by a constant bias.

We set $7$ simulated 3D point-features about $3$m away from the MAV, which flies at a speed of around $2~m~s^{-1}$.
We found that setting the frame rate of the simulated camera at 10Hz provides a sufficient pixel disparity with the following setup.
In practice, increasing the frame rate above 30Hz decreases the pixel disparity and introduces numerical instability for this setup.
The theoretical cases in which our system admits singularities are provided in \cite{Martinelli2012, Martinelli2014}.
Reducing the number of considered frames also reduces the size of the matrices and, thus, speeds up the computations.
As an example, over a time interval of 3 seconds, we obtain $31$ distinct frames.
When observing 7 features, solving the closed-form solution is equivalent to inverting a linear system of $3\times 30\times 7 = 630$ equations and $6+7\times 31=223$ unknowns (see section \ref{SectionCFS}).

The method we use to solve the overconstrained linear system $\Xi X = S$ is a Singular Value Decomposition (SVD) since it yields numerically robust solutions.

In the next section, we will present the results obtained with the original closed-form solution on the simulated data mentioned, with different sensor bias settings.
Our goal is to identify its performance limitations and introduce modifications to overcome them.


%% The case of a biased gyroscope is addressed in chapter \ref{ch:gyro}.
%% Indeed, the main contribution of this report is the simple method introduced in Section \ref{sec:computeBias} to compute the gyroscope bias with the closed-form solution, which happened to be a major performance bottleneck.

\subsection{Performance without bias}

The original closed-form solution described in Equation (\ref{eq:mat1}) will be used as a basis for our work.
Moreover, we can also use the knowledge of the gravity magnitude to refine our results (Equation (\ref{eq:mat2})).
In this case, we are minimizing a linear objective function with a quadratic constraint.
In Fig. \ref{fig:original}, we display the performance of the original Closed-Form (CF) solution in estimating speed, gravity in the local frame, and distances to the features with and without this additional constraint.

\begin{figure}
  \centering
    \resizebox{0.7\columnwidth}{!}{\input{graph/originalCF}}
    \caption{Original closed-form solution estimations with and without using the knowledge of the gravity (\ref{eq:mat2}). We are observing 7 features  over a variable duration of integration.\label{fig:original}}
\end{figure}

Note how the evaluations get better as we increase the integration time.
Indeed, our equations come from an extended triangulation \cite{Martinelli2012}.
Therefore, it requires a significant difference in the measurements over time to robustly estimate the state.


Without sensor bias, the original closed-form robustly estimates all the properties (below $0.1\%$ error) after $2$ seconds of integration.

Note that a robust estimation of the gravity requires a shorter duration of integration than the speed and the distance to the features.
In general, we found that the gravity is well estimated with the original closed-form solution due to its strong weight in the equations (see section \ref{sec:biasAcc}).
Therefore, constraining its magnitude does not improve the performance much.
In the following sections, we remove this constraint.



\subsection{Impact of accelerometer bias on the performance \label{sec:biasAcc}}
In order to visualize the impact of the accelerometer bias on the performance,
we corrupt the accelerometer measurements by a bias (Fig. \ref{fig:biasAccCF}).
Despite a high accelerometer bias, the closed-form solution still provides robust results.
As seen in Fig. \ref{fig:biasAccCF}, neither the estimation of the gravity, the velocity or the lambdas is impacted by the accelerometer bias.
To explain this behavior we ran many simulations by also considering trajectories that are not plausible for an MAV and by also changing the magnitude of the gravity. We found the following conclusions. When the rotations are small, the effect of a bias is negligible even if its value is larger than the inertial acceleration. This is easily explained by remarking that, in the case of negligible rotations, a bias on the accelerometer acts as the gravity. Hence, its impact depends on the ratio between its magnitude and the magnitude of the gravity.
If the rotations are important, the effect of a bias on the accelerometer is negligible when its magnitude is smaller than both the gravity and the inertial acceleration. Note that, for an MAV that accomplishes a loop of radius $1m$ and speed $2~m~s^{-1}$, the inertial acceleration is $4~m~s^{-2}$.

In \cite{Martinelli2014}, the author provides an alternative formulation of the closed-form solution including the accelerometer bias as an observable unknown of the system.
However, the estimation of the accelerometer bias with that method is not robust since our system is only slightly affected by it\footnote{Additionally, in \cite{Martinelli2014} property 12, we prove that rotations must occur around at least two independent axes to determine the bias. In general, for a motion of a few seconds, an MAV accomplishes rotations around a single axis.}.

\begin{figure}
  \centering
        \hspace{0.2\columnwidth}%
        \begin{subfigure}[b]{0.3\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasLegend}}
                \caption{Legend}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasGravity}}
                \caption{Gravity estimation error}

        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasSpeed}}
                \caption{Velocity estimation error}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/accBiasLambda}}
                \caption{Lambda estimation error}

        \end{subfigure}
        \caption{Impact of the accelerometer bias on the performance of the closed-form solution. We are observing 7 features  over a variable duration of integration.\label{fig:biasAccCF}}
\end{figure}



\subsection{Impact of gyroscope bias on the performance}
To visualize the impact of the gyroscope bias on the performance,
we corrupt the gyroscope measurements by an artificial bias (Fig. \ref{fig:biasGyroCF}).

%Our experiments have shown that the presence of gyroscope bias significantly damages the results of the closed-form solution.

\begin{figure}
  \centering
        \hspace{0.2\columnwidth}%
        \begin{subfigure}[b]{0.3\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasLegend}}
                \caption{Legend}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasGravity}}
                \caption{Gravity estimation error}

        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasSpeed}}
                \caption{Velocity estimation error}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasLambda}}
                \caption{Lambda estimation error}

        \end{subfigure}
        \caption{Impact of the gyroscope bias on the performance of the closed-form solution. We are observing 7 features over a variable duration of integration. \label{fig:biasGyroCF}}
\end{figure}


As seen in Fig. \ref{fig:biasGyroCF}, the performance becomes very poor in presence of a bias on the gyroscope and, in practice, the overall method could only be successfully used with a very precise---and expensive---gyroscope.

Note that, in \cite{Martinelli2014}, the author evaluates the performance of the closed-form solution with a simulated gyroscope bias of magnitude $0.5deg/s \approx 0.0087 rad/s$.
In Fig. \ref{fig:biasGyroCF}, this bias would yield a curve between the green and the blue ones, with relative error below $10\%$.


\section{ESTIMATING THE GYROSCOPE BIAS}\label{SectionCalibration}

Previous work has shown that the gyroscope bias is an observable mode when using an IMU and a camera, which means that it can be estimated \cite{Martinelli2012}.
In this section, we propose an optimization approach to estimate the gyroscope bias using the closed-form solution.

%
%Optimally, we would add the gyroscope bias in our unknown vector $X$ and determine $X$ by simply inverting the system $\Xi X = S$ as in the standard closed-form solution.
%However, we cannot express the gyroscope bias linearly with this system.


\subsection{Nonlinear minimization of the residual}

Since our system of equations (\ref{eq:final1}) is overconstrained,
inverting it is equivalent to finding the vector $X$ that minimizes the residual $||\Xi X - S||^2$.
We define the following cost function:

\begin{equation}
  \label{eq:cost}
  cost(B) = ||\Xi X - S||^2,
\end{equation}

\noindent with:
\begin{itemize}
\item $B$ the gyroscope bias;
\item $\Xi$ and $S$ computed by replacing the angular velocity provided by the gyroscope $\omega$ by $\omega - B$.
\end{itemize}

By minimizing this cost function, we recover the gyroscope bias $B$ and the unknown vector $X$.
%which has accounted for the gyroscope bias $B$.
Since our cost function requires an initialization and is non-convex (see Fig. \ref{fig:cost}),
the optimization process can be stuck in local minima.
However, by running extensive simulations we found that the cost function is convex around the true value of the bias. Hence, we can initialize the optimization process with $B = 0_3$ since the bias is usually rather small.



\begin{figure}
  \centering
        \hspace{0.2\columnwidth}%
        \begin{subfigure}[b]{0.3\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/gyroBiasLegend}}
                \caption{Legend}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/opt7ftGravity}}
                \caption{Gravity estimation error}

        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/opt7ftSpeed}}
                \caption{Velocity estimation error}

        \end{subfigure}%
        \begin{subfigure}[b]{0.5\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/opt7ftLambda}}
                \caption{Lambda estimation error}

        \end{subfigure}
        \caption{Impact of the gyroscope bias on the performance of the optimized closed-form solution. We are observing 7 features  over a variable duration of integration. \label{fig:biasGyroOpt}}
\end{figure}

\begin{figure}
  \centering
  \resizebox{0.7\columnwidth}{!}{\input{graph/finals/gyroEval0.1}}
  \caption{Gyroscope bias estimation from nonlinear minimization of the residual.
    We are observing 7 features over a variable duration of integration.
    The true bias is $B=[-0.0170, -0.0695, 0.0698]$ with magnitude $||B||=0.1$ and the final bias estimate is $[-0.0183, -0.0697, 0.0708]$.\label{fig:gBiasEstimate}}
\end{figure}

%% This technique is similar to the one used in \cite{Abbeel2005}, where the author minimizes the residual of a Kalman filter in order to recover the noise matrix for the sensors.

%% Moreover, the optimized closed-form solution requires a shorter integration duration to provide good quality results.
%% Specifically, after 2 seconds of integration (around 7 frames) the provided estimations are already robust.
%% The non-optimized closed-form solution requires 3 seconds of integration before converging to acceptable estimations.


As seen in Fig. \ref{fig:gBiasEstimate}, this method can robustly estimate high values of the gyroscope bias (relative error of final bias estimate is below $2\%$).
Fig. \ref{fig:biasGyroOpt} displays the performance of the proposed method in estimating speed, gravity in the local frame, and distances to the features in presence of the same artificial gyroscope bias from Fig. \ref{fig:biasGyroCF}.
As seen in Fig. \ref{fig:biasGyroOpt}, after $1s$ of integration duration, the estimations agree no matter how high the bias is.
In other words, given that the integration duration is long enough, this method is unaffected by the gyroscope bias.
Using Levenberg-Marquardt algorithm, the optimization process reaches its optimal value after around $4$ iterations and $20$ evaluations of the cost function.
Evaluating the cost function is equivalent to solving the linear system described in Equation (\ref{eq:mat1}).

\noindent For very short time of integration ($<1$ second), the cost function loses its local convexity and the proposed method can fail by providing a gyroscope bias much larger than the correct one.
To understand this misestimation, in Fig. \ref{fig:cost} we plot the residual with respect to the bias, which is the cost function we are minimizing.
We highlight a misestimation of the gyroscope bias by setting the duration of integration to 1 second while observing 7 features.
We refer to the components of the gyroscope bias by $B = [B_x, B_y, B_z]$.
As we can see in Fig. \ref{fig:cost}, the cost function admits a symmetry with respect to $B_z$ (and consequently it is not convex).
This symmetry replicates the minima of the true gyroscope bias along $B_z$.
The optimization process can therefore diverge from the true gyroscope bias.
In the next section, we present a method to use a priori knowledge to guide the optimization process.

\begin{figure}
        \centering
        \begin{subfigure}[b]{0.489\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/costXZ}}
                \caption{Residual with respect to $B_x$ and $B_z$}
        \end{subfigure}~
        \begin{subfigure}[b]{0.489\columnwidth}
                \resizebox{\columnwidth}{!}{\input{graph/costYZ}}
                \caption{Residual with respect to $B_y$ and $B_z$}
        \end{subfigure}
        \caption{Cost function (residual) with respect to the gyroscope bias for a small amount of available measurements (integration of 1 second while observing 7 features)\label{fig:cost}}
\end{figure}

\subsection{Removing the symmetry in the cost function \label{sec:symmetry}}

The symmetry in the cost function is induced by the strong weight of the gravity in the Equation (\ref{eq:final1}).
In general, the residual is almost constant with respect to the component of the gyroscope bias along the direction $\vec{u}$ when $\vec{u}$ is collinear with the gravity throughout the motion.
Since an MAV normally operates in near-hover conditions, $\vec{u}$ is approximated to the vector pointing upward in the gyroscope frame when the MAV is hovering.
If the MAV rotates such that $\vec{u}$ becomes noncollinear with the gravity, the cost function does not exhibit this symmetry anymore.
In this case, the gyroscope bias is well estimated.
A simple solution to avoid having that symmetry in our system would be to enforce that there is no such $\vec{u}$ by forcing our MAV to perform rotations while it is operating.
Another way to artificially get rid of this symmetry is to tweak the cost function.
Specifically, we can add a regularization term that penalizes high estimations of the component of the bias along $\vec{u}$:

\begin{equation}
cost(B) = ||\Xi X - S||^2 + \lambda (\vec{u} \cdot B)^2,
\end{equation}


\noindent with $\vec{u}$ the direction collinear with the gravity throughout the motion and  $\lambda$ the coefficient given to how much we want to penalize this bias component.



%\noindent with:
%\begin{itemize}
%\item $\vec{u}$ the direction collinear with the gravity throughout the motion;
%\item $\lambda$ the coefficient given to how much we want to penalize this bias component.
%\end{itemize}

For small values of $\lambda$, our cost function is similar to the previous one and the bias can grow arbitrarily high.
Note that, instead of forcing this gyroscope bias component to be close to $0$, we can easily force it to be close to any value.
Therefore, we can use the a priori knowledge of a gyroscope bias approximation:

\[
cost(B) = ||\Xi X - S||^2 +  \lambda (\vec{u} \cdot (B - B^{approx} ))^2,
\]

\noindent with $B^{approx}$ being the known approximate gyroscope bias.
This methods allows us to reuse previously-computed gyroscope bias since it is known to slowly vary over time.
The value of $\lambda$ should be set starting from the knowledge about the range of change of the gyroscope bias.
We can obtain this variation with previously-computed gyroscope bias.

\section{EXPERIMENTS ON REAL DATA}\label{SectionPerformance}

We validate our method on a real dataset containing IMU and camera measurements from a flying quadrotor along with ground truth.

\subsection{Experimental setup}\label{SubsectionSetup}

For our evaluation, we consider an MAV flying in a room equipped with a motion-capture system.
This allows us to compare the estimations of the velocity along with the roll and pitch angles against ground truth.

\begin{figure}
  \centering
  %% \begin{subfigure}[t]{\columnwidth}
  %%   \includegraphics[width=\textwidth]{images/setupTestDroneError.png}
  %%   \caption{Sketch of the experimental setup}
  %% \end{subfigure}
  %% \vspace{0.5cm}

  \begin{subfigure}[t]{0.489\columnwidth}
  \includegraphics[width=\textwidth]{images/quadrotor_closeup.jpg}
  \caption{A closeup of our quadrotor: 1) down-looking camera,
2) Odroid U3 quad-core computer, 3) PIXHAWK autopilot.\label{fig:droneCloseup}}
  \end{subfigure}~
  \begin{subfigure}[t]{0.489\columnwidth}
  \includegraphics[width=\textwidth, trim={13.2cm 2cm 0 4cm}, clip]{images/realExperiment.png}
  \caption{Our flying arena equipped with an OptiTrack motion-capture system (for ground-truth recording).\label{fig:droneFly}}
  \end{subfigure}
  %% \begin{subfigure}[t]{0.489\columnwidth}
  %% \includegraphics{images/virtualExperiment.png}
  %% \caption{}
  %% \end{subfigure}
  \caption{Experimental setup for identifying the limitations of the performance.
    The drone is equipped with an IMU and a down-looking camera.
    \label{fig:testsetup}}
\end{figure}

We use the same MAV used in \cite{FaesslerICRA15}, Section 3.4.
Specifically, our quadrotor relies on the frame of the Parrot AR.Drone 2.0 including their motors, motor controllers, gears, and propellers.
It is equipped with a PX4FMU autopilot and a PX4IOAR adapter board.
The PX4FMU includes a 200Hz IMU.
The MAV is also equipped with a downward-looking MatrixVision mvBlueFOX-MLC200w ($752 \times 480$-pixel) monochrome camera with a 130-degree field-of-view lens (Fig. \ref{fig:droneCloseup}).
The data are recorded using an Odroid-U3 single-board computer.
The MAV flies indoors at low altitude ($1.5$m) (Fig. \ref{fig:droneFly}).
The feature extraction and matching is done via the FAST corners \cite{Rosten2005, Rosten2006}.

\subsection{Results}

We compare the performance on the estimations of the gravity and velocity obtained with three methods:
\begin{itemize}
\item The original closed-form solution \cite{Martinelli2014} (Equation \ref{eq:mat1});
\item Our modified closed-form solution (this paper) (Equation \ref{eq:cost});
\item The loosely-coupled visual-inertial algorithm (MSF) in \cite{LynenIROS13} using pose estimates from the Semi-direct Visual Odometry (SVO) package \cite{Forster2014} (how to combine MSF \cite{LynenIROS13} with SVO can be found in \cite{FaesslerICRA15}).
\end{itemize}
The reason we included SVO+MSF in the validation is to have a reference state-of-the-art pose estimation method.
However, MSF requires to be initialized with a rough absolute scale, whereas our method works without initialization.
We set the integration duration for the closed-form solution to 2.8 seconds, since it is sufficient to obtain robust results (see Fig. \ref{fig:gBiasEstimate}).
The camera provides 60fps, but we discard most of the frames and consider only 10Hz (this is discussed in section \ref{SubsectionConsidered}).
As seen in Fig. \ref{fig:valid}, the performance obtained by our method is similar than the performance obtained by a well-initialized MSF.
We remind the reader that unlike MSF, the closed-form solution does not require the knowledge of the absolute scale to be provided.
Moreover, the original closed-form solution and the optimized closed-form solution have similar performance.
Indeed, for this dataset the gyroscope bias was estimated to $B = [0.0003, 0.009, 0.001]$, which is very small $(||B|| = 0.0091)$.
To prove the robustness of our method compared to the original closed-form,
we corrupt the gyroscope measurements provided by the dataset with an artificial bias in Fig. \ref{fig:validBias1} and Fig. \ref{fig:validBias2}.
As seen in these figures, our method is robust against gyroscope bias whereas the original closed-form is not.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.7\columnwidth}
   \resizebox{\columnwidth}{!}{\input{graph/results2.8s-CF3-CF3opt-KF.tex}}
    \caption{No artificial bias.\label{fig:valid}}
  \end{subfigure}
  \begin{subfigure}[b]{0.489\columnwidth}
    \resizebox{\columnwidth}{!}{\input{graph/results2.8s-CF3-CF3Opt-Bias0.05.tex}}
    \caption{Artificial bias of magnitude $0.05 rad/s$.\label{fig:validBias1}}
  \end{subfigure}
  \begin{subfigure}[b]{0.489\columnwidth}
    \resizebox{\columnwidth}{!}{\input{graph/results2.8s-CF3-CF3Opt-Bias0.1.tex}}
    \caption{Artificial bias of magnitude $0.1 rad/s$.\label{fig:validBias2}}
  \end{subfigure}
  \caption{Estimation error of the optimized closed-form solution against the original closed-form solution \cite{Martinelli2014} and SVO \cite{FaesslerICRA15}.
    The duration of integration is set to 2.8 seconds, and 10 point features are observed throughout the whole operation.
    In Fig. \ref{fig:validBias1} and Fig. \ref{fig:validBias1}, we corrupted the gyroscope measurements with an artificial bias.}

\end{figure}

%In a real applications, a proper online computation of the gyroscope bias can be reused.
%% Since the gyroscope bias varies slowly over time, we can reuse its knowledge.
%% In other words, we can use the optimized closed-form solution to compute the bias at the beginning of the sequence.
%% Thereafter, we can quickly provide reliable state estimations with the standard closed-form solution by compensating with the computed bias.

%% For long term navigation (longer than a minute), it would be advised to recompute the gyroscope bias every so often.
%% This re-computation can use the knowledge of previously computed bias. % as stated in Section \ref{sec:reg}.

%% The reason why we do not run the optimization for every state estimation is to speed up the computation.
%% Despite the optimization is relatively fast since our cost function is defined only with respect to the gyroscope bias, it still has to run the closed-form solution several times for its gradient descent to converge.
%% We found by experimentation that it usually requires around 6 iterations to converge.


\section{CONCLUSION}\label{SectionConclusion}

%For an MAV, limiting the number of on-board sensor is important to save power consumption and processing power.
%A popular choice of sensors is a camera coupled with an IMU for their complementary.
%However, the methods for fusing visual and inertial measurements so far introduced are filter based, hence require an initialization.
%Providing a reliable state initialization is critical for these algorithms to work correctly.

In this paper, we studied the recent closed-form solution proposed by \cite{Martinelli2014} which performs visual-inertial sensor fusion without requiring an initialization.
We implemented this method in order to test it with plausible MAV motions and synthetic noisy sensor data.
This allowed us to identify its performance limitations and bring modifications to overcome them.
We investigated the impact of biased inertial measurements.
Although the case of biased accelerometer was originally studied in \cite{Martinelli2014}, we showed that the accelerometer bias does not significantly worsen the performance.
One major performance limitation of this method was due to the impact of biased gyroscope measurements.
In other words, the performance becomes very poor in presence of a bias on the gyroscope and, in practice, the overall method could only be successfully used with a very precise (and expensive) gyroscope.
We then introduced a simple method that automatically estimates this bias.
We validated this method by comparing its performance against state-of-the-art pose estimation approach for MAV.
For future work, we see this optimized closed-form solution being used on an MAV to provide accurate state initialization.
This would allow aggressive take-off maneuvers, such as hand throwing the MAV in the air, as already demonstrated in \cite{Faessler2015} with a range sensor.
With our technique, we could get rid of the range sensor.

\printbibliography

\end{document}
